{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:07.850183Z",
     "iopub.status.busy": "2025-02-16T14:52:07.849957Z",
     "iopub.status.idle": "2025-02-16T14:52:11.130047Z",
     "shell.execute_reply": "2025-02-16T14:52:11.129141Z",
     "shell.execute_reply.started": "2025-02-16T14:52:07.850155Z"
    },
    "id": "JHSdxPJuN4x7",
    "outputId": "a74dc274-dd24-4415-a3e4-1fcff4e437a2",
    }
   },
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ],
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.132700Z",
     "iopub.status.busy": "2025-02-16T14:52:11.132104Z",
     "iopub.status.idle": "2025-02-16T14:52:11.136301Z",
     "shell.execute_reply": "2025-02-16T14:52:11.135528Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.132665Z"
    },
    "id": "yQQEArYqWylq",
    "outputId": "bc80eeee-442a-4ff2-f69e-348314f84bce",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.484486Z",
     "start_time": "2025-02-17T03:03:37.479399Z"
    }
   },
   "source": [
    "#挂载谷歌云盘\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.137477Z",
     "iopub.status.busy": "2025-02-16T14:52:11.137124Z",
     "iopub.status.idle": "2025-02-16T14:52:11.141281Z",
     "shell.execute_reply": "2025-02-16T14:52:11.140372Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.137447Z"
    },
    "id": "wzS4AimwWz7f",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.501498Z",
     "start_time": "2025-02-17T03:03:37.488500Z"
    }
   },
   "source": [
    "# !cp /content/drive/MyDrive/transformer-de-en/* . -r"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55aNBForN4x9"
   },
   "source": [
    "## 数据加载\n",
    "\n",
    "- 采用WMT16的德语和英语平行语料库，数据集主页：[WMT16](https://www.statmt.org/wmt16/multimodal-task.html#task1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.142645Z",
     "iopub.status.busy": "2025-02-16T14:52:11.142280Z",
     "iopub.status.idle": "2025-02-16T14:52:11.145974Z",
     "shell.execute_reply": "2025-02-16T14:52:11.145165Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.142607Z"
    },
    "id": "SkjBEbr1N4x-",
    "outputId": "4b9817d0-616b-41a0-ceb7-9ff6bf0f42a1",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.511394Z",
     "start_time": "2025-02-17T03:03:37.503045Z"
    }
   },
   "source": [
    "#和jieba分词类似\n",
    "# !pip install sacremoses\n",
    "# !pip install subword-nmt\n",
    "# # BPE分词"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.147391Z",
     "iopub.status.busy": "2025-02-16T14:52:11.146872Z",
     "iopub.status.idle": "2025-02-16T14:52:11.279554Z",
     "shell.execute_reply": "2025-02-16T14:52:11.278546Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.147346Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.577004Z",
     "start_time": "2025-02-17T03:03:37.513404Z"
    }
   },
   "source": [
    "!pwd"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.281204Z",
     "iopub.status.busy": "2025-02-16T14:52:11.280809Z",
     "iopub.status.idle": "2025-02-16T14:52:11.285262Z",
     "shell.execute_reply": "2025-02-16T14:52:11.284540Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.281171Z"
    },
    "id": "Tc8aCQX-XrGv",
    "outputId": "67964082-2737-4d47-dd66-455e4ae69660",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.582181Z",
     "start_time": "2025-02-17T03:03:37.578011Z"
    }
   },
   "source": [
    "# !sh data_multi30k.sh wmt16 wmt16_cut de en"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lvk4q0-1N4x-"
   },
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.289105Z",
     "iopub.status.busy": "2025-02-16T14:52:11.288579Z",
     "iopub.status.idle": "2025-02-16T14:52:11.441596Z",
     "shell.execute_reply": "2025-02-16T14:52:11.440701Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.289078Z"
    },
    "id": "zCXKEwspN4x_",
    "outputId": "3dcce6fa-932f-4b22-9dc8-398a4b402733",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.760343Z",
     "start_time": "2025-02-17T03:03:37.585188Z"
    }
   },
   "source": [
    "from pathlib import Path  # 用于处理文件路径\n",
    "from torch.utils.data import Dataset, DataLoader  # 导入 PyTorch 数据集和数据加载器\n",
    "\n",
    "class LangPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    该类用于加载和处理双语数据集（如德语-英语），并可选择将数据缓存到本地，避免重复处理。\n",
    "    \"\"\"\n",
    "    def __init__(self, mode=\"train\", max_length=128, overwrite_cache=False, data_dir=\"wmt16\"):\n",
    "        \"\"\"\n",
    "        初始化数据集。\n",
    "        :param mode: 指定数据集模式（\"train\" 或 \"val\"）\n",
    "        :param max_length: 句子的最大长度，超过该长度的句子会被过滤掉\n",
    "        :param overwrite_cache: 是否覆盖缓存文件，默认为 False\n",
    "        :param data_dir: 数据存放的目录\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)  # 设定数据存储路径\n",
    "        cache_path = self.data_dir / \".cache\" / f\"de2en_{mode}_{max_length}.npy\"  # 缓存文件路径\n",
    "        \n",
    "        if overwrite_cache or not cache_path.exists():  # 如果选择覆盖缓存或缓存文件不存在\n",
    "            cache_path.parent.mkdir(parents=True, exist_ok=True)  # 创建缓存目录（如果不存在）\n",
    "            \n",
    "            # 读取源语言文件（.bpe 文件格式）\n",
    "            with open(self.data_dir / f\"{mode}_src.bpe\", \"r\", encoding=\"utf8\") as file:\n",
    "                self.src = file.readlines()  # 读取所有行，存入列表\n",
    "            \n",
    "            # 读取目标语言文件（.bpe 文件格式）\n",
    "            with open(self.data_dir / f\"{mode}_trg.bpe\", \"r\", encoding=\"utf8\") as file:\n",
    "                self.trg = file.readlines()  # 读取所有行，存入列表\n",
    "            \n",
    "            filtered_src = []  # 存放符合长度要求的源语言句子\n",
    "            filtered_trg = []  # 存放符合长度要求的目标语言句子\n",
    "            \n",
    "            # 遍历源语言和目标语言句子，进行长度过滤\n",
    "            for src, trg in zip(self.src, self.trg):\n",
    "                if len(src) <= max_length and len(trg) <= max_length:  # 只保留长度符合要求的句子\n",
    "                    filtered_src.append(src.strip())  # 去除首尾空格并添加到列表\n",
    "                    filtered_trg.append(trg.strip())\n",
    "            \n",
    "            # 转换为 NumPy 数组\n",
    "            filtered_src = np.array(filtered_src)\n",
    "            filtered_trg = np.array(filtered_trg)\n",
    "            \n",
    "            # 将过滤后的数据保存为缓存文件\n",
    "            np.save(cache_path, {\"src\": filtered_src, \"trg\": filtered_trg}, allow_pickle=True) # allow_pickle=True 为了保存字典\n",
    "            print(f\"save cache to {cache_path}\")  # 提示缓存已保存\n",
    "        \n",
    "        else:  # 如果缓存文件存在，则直接加载缓存数据\n",
    "            cache_dict = np.load(cache_path, allow_pickle=True).item()  # 读取缓存文件\n",
    "            print(f\"load {mode} dataset from {cache_path}\")  # 提示数据已加载\n",
    "            filtered_src = cache_dict[\"src\"]  # 读取源语言数据\n",
    "            filtered_trg = cache_dict[\"trg\"]  # 读取目标语言数据\n",
    "        \n",
    "        self.src = filtered_src  # 存储源语言数据\n",
    "        self.trg = filtered_trg  # 存储目标语言数据\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取索引对应的源语言和目标语言句子。\n",
    "        :param index: 数据索引\n",
    "        :return: (源语言句子, 目标语言句子)\n",
    "        \"\"\"\n",
    "        return self.src[index], self.trg[index]  # 返回对应索引的数据\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集的大小。\n",
    "        \"\"\"\n",
    "        return len(self.src)  # 数据集大小等于源语言句子的数量\n",
    "\n",
    "# 创建训练集和验证集的数据集对象\n",
    "train_ds = LangPairDataset(\"train\")  # 训练集\n",
    "val_ds = LangPairDataset(\"val\")  # 验证集"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wmt16\\\\train_src.bpe'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 71\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msrc)  \u001B[38;5;66;03m# 数据集大小等于源语言句子的数量\u001B[39;00m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;66;03m# 创建训练集和验证集的数据集对象\u001B[39;00m\n\u001B[1;32m---> 71\u001B[0m train_ds \u001B[38;5;241m=\u001B[39m \u001B[43mLangPairDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 训练集\u001B[39;00m\n\u001B[0;32m     72\u001B[0m val_ds \u001B[38;5;241m=\u001B[39m LangPairDataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# 验证集\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[7], line 23\u001B[0m, in \u001B[0;36mLangPairDataset.__init__\u001B[1;34m(self, mode, max_length, overwrite_cache, data_dir)\u001B[0m\n\u001B[0;32m     20\u001B[0m cache_path\u001B[38;5;241m.\u001B[39mparent\u001B[38;5;241m.\u001B[39mmkdir(parents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# 创建缓存目录（如果不存在）\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# 读取源语言文件（.bpe 文件格式）\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmode\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_src.bpe\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msrc \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mreadlines()  \u001B[38;5;66;03m# 读取所有行，存入列表\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# 读取目标语言文件（.bpe 文件格式）\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'wmt16\\\\train_src.bpe'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.442905Z",
     "iopub.status.busy": "2025-02-16T14:52:11.442524Z",
     "iopub.status.idle": "2025-02-16T14:52:11.446414Z",
     "shell.execute_reply": "2025-02-16T14:52:11.445675Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.442874Z"
    },
    "id": "yHB9TDpDQlv2",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.763351Z",
     "start_time": "2025-02-17T03:03:37.763351Z"
    }
   },
   "source": [
    "# !rm wmt16/.cache -r"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.447804Z",
     "iopub.status.busy": "2025-02-16T14:52:11.447331Z",
     "iopub.status.idle": "2025-02-16T14:52:11.452304Z",
     "shell.execute_reply": "2025-02-16T14:52:11.451360Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.447777Z"
    },
    "id": "meHHXL3MN4x_",
    "outputId": "a001c071-7021-4dff-8bc0-3e5da01d7210",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.765351Z",
     "start_time": "2025-02-17T03:03:37.765351Z"
    }
   },
   "source": [
    "print(len(train_ds)) # 少了1000多个样本\n",
    "print(\"source: {}\\ntarget: {}\".format(*train_ds[-1]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE8gRzYQN4yA"
   },
   "source": [
    "### Tokenizer\n",
    "\n",
    "这里有两种处理方式，分别对应着 encoder 和 decoder 的 word embedding 是否共享，这里实现共享的方案"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "7190c928edb040c3a523cdd2e3f45572",
      "990741ff083d457587665d8f620b69c3",
      "00fe6a550df547cea5b1e0d84c1a3f5f",
      "fab8b1e3187841ee894b92859fa4821b",
      "f9b0d916e1a64c2a9f384e42f3d5dcab",
      "a78199650ecc47bd81f482a2671b563a",
      "673bd3773a374261a01cfc48081c27f3",
      "83f345f7424c43929d7a304cbfb6d0fe",
      "58d69d086f644e89937347e1f9261d6f",
      "8cf462fa996c41b18fb4571e0a757363",
      "434ac4c51db74681942d2d3bd869b1df"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.453497Z",
     "iopub.status.busy": "2025-02-16T14:52:11.453170Z",
     "iopub.status.idle": "2025-02-16T14:52:11.487851Z",
     "shell.execute_reply": "2025-02-16T14:52:11.487124Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.453471Z"
    },
    "id": "yAmlq_9YN4yA",
    "outputId": "263a89f7-9bee-4c6d-cf7a-7d82b0a81f92",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.766351Z",
     "start_time": "2025-02-17T03:03:37.766351Z"
    }
   },
   "source": [
    "# 载入词表，看下词表长度，词表就像英语字典, 构建 word2idx 和 idx2word\n",
    "\n",
    "word2idx = {\n",
    "    \"[PAD]\": 0,     # 填充 token，表示句子长度不足时填充的占位符\n",
    "    \"[BOS]\": 1,     # 句子起始 token，Begin Of Sentence\n",
    "    \"[UNK]\": 2,     # 未知 token，Unknown，表示词表中未收录的词\n",
    "    \"[EOS]\": 3,     # 句子结束 token，End Of Sentence\n",
    "}\n",
    "\n",
    "# 构建索引到单词的映射，即 idx2word，方便通过索引查找对应单词\n",
    "idx2word = {value: key for key, value in word2idx.items()}\n",
    "\n",
    "# 记录当前索引的起始位置，从 4 开始（因为已有 4 个特殊 token）\n",
    "index = len(idx2word)\n",
    "\n",
    "# 设定一个阈值，只有出现次数大于等于该阈值的 token 才会加入词表\n",
    "threshold = 1  \n",
    "\n",
    "# 读取词表文件，词表文件格式为每行一个单词及其出现次数\n",
    "with open(\"wmt16/vocab\", \"r\", encoding=\"utf8\") as file:\n",
    "    for line in tqdm(file.readlines()):  # 使用 tqdm 显示进度条，适用于大词表\n",
    "        token, counts = line.strip().split()  # 按空格拆分，每行格式为 \"单词 词频\"\n",
    "        if int(counts) >= threshold:  # 只有词频大于等于 threshold 才加入词表\n",
    "            word2idx[token] = index  # 给该单词分配索引\n",
    "            idx2word[index] = token  # 反向索引表\n",
    "            index += 1  # 更新索引计数\n",
    "\n",
    "# 计算最终词表大小（即 word2idx 的键值对个数）\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab_size: {}\".format(vocab_size))  # 打印词表大小"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.489116Z",
     "iopub.status.busy": "2025-02-16T14:52:11.488792Z",
     "iopub.status.idle": "2025-02-16T14:52:11.501277Z",
     "shell.execute_reply": "2025-02-16T14:52:11.500401Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.489090Z"
    },
    "id": "e_ed0jmJN4yA",
    "outputId": "407a9e69-4981-42ae-c7a9-aa4c701185ef",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.768350Z",
     "start_time": "2025-02-17T03:03:37.767350Z"
    }
   },
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word2idx, idx2word, max_length=128, pad_idx=0, bos_idx=1, eos_idx=3, unk_idx=2):\n",
    "        \"\"\"\n",
    "        初始化 Tokenizer\n",
    "\n",
    "        :param word2idx: 单词到索引的映射\n",
    "        :param idx2word: 索引到单词的映射\n",
    "        :param max_length: 句子最大长度，超过会被截断，默认 128\n",
    "        :param pad_idx: 填充 token 的索引，默认为 0（[PAD]）\n",
    "        :param bos_idx: 句子起始 token 的索引，默认为 1（[BOS]）\n",
    "        :param eos_idx: 句子结束 token 的索引，默认为 3（[EOS]）\n",
    "        :param unk_idx: 未知单词的索引，默认为 2（[UNK]）\n",
    "        \"\"\"\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.max_length = max_length\n",
    "        self.pad_idx = pad_idx\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.unk_idx = unk_idx\n",
    "\n",
    "    def encode(self, text_list, padding_first=False, add_bos=True, add_eos=True, return_mask=False):\n",
    "        \"\"\"\n",
    "        将文本列表编码为索引列表，并进行填充或截断。\n",
    "\n",
    "        :param text_list: 需要编码的文本列表（每个元素都是分词后的单词列表）\n",
    "        :param padding_first: 是否在前面填充 [PAD]，默认为 False（即填充在后面）\n",
    "        :param add_bos: 是否添加 [BOS] 作为起始符号，默认为 True\n",
    "        :param add_eos: 是否添加 [EOS] 作为结束符号，默认为 True\n",
    "        :param return_mask: 是否返回 mask（掩码），用于计算损失时忽略 [PAD] 位置\n",
    "        :return: 若 return_mask=False，则返回编码后的 input_ids；否则返回 (input_ids, masks)\n",
    "        \"\"\"\n",
    "\n",
    "        # 计算句子最大长度，确保不会超过 max_length\n",
    "        max_length = min(self.max_length, add_eos + add_bos + max([len(text) for text in text_list]))\n",
    "\n",
    "        indices_list = []  # 存放编码后的索引序列\n",
    "        for text in text_list:\n",
    "            # 将单词转换为索引，遇到未登录词（OOV）则替换为 [UNK]\n",
    "            indices = [self.word2idx.get(word, self.unk_idx) for word in text[:max_length - add_bos - add_eos]]\n",
    "\n",
    "            # 根据参数，决定是否在索引列表前后添加 [BOS] 和 [EOS]\n",
    "            if add_bos:\n",
    "                indices = [self.bos_idx] + indices\n",
    "            if add_eos:\n",
    "                indices = indices + [self.eos_idx]\n",
    "\n",
    "            # 进行填充（padding），使所有句子长度一致\n",
    "            if padding_first:\n",
    "                # 在前面填充 [PAD]\n",
    "                indices = [self.pad_idx] * (max_length - len(indices)) + indices\n",
    "            else:\n",
    "                # 在后面填充 [PAD]\n",
    "                indices = indices + [self.pad_idx] * (max_length - len(indices))\n",
    "\n",
    "            indices_list.append(indices)  # 加入到结果列表\n",
    "\n",
    "        # 转换为 PyTorch 张量\n",
    "        input_ids = torch.tensor(indices_list)\n",
    "\n",
    "        # 生成 mask，填充部分（[PAD]）标记为 1，其余部分标记为 0\n",
    "        masks = (input_ids == self.pad_idx).to(dtype=torch.int64)\n",
    "\n",
    "        # 返回编码后的 input_ids，或 (input_ids, masks)\n",
    "        return input_ids if not return_mask else (input_ids, masks)\n",
    "\n",
    "    def decode(self, indices_list, remove_bos=True, remove_eos=True, remove_pad=True, split=False):\n",
    "        \"\"\"\n",
    "        将索引列表解码回文本列表。\n",
    "\n",
    "        :param indices_list: 需要解码的索引列表（可包含多个句子）\n",
    "        :param remove_bos: 是否移除 [BOS] 令牌，默认为 True\n",
    "        :param remove_eos: 是否移除 [EOS] 令牌，默认为 True\n",
    "        :param remove_pad: 是否移除 [PAD] 令牌，默认为 True\n",
    "        :param split: 是否返回分词后的列表，默认为 False（即返回拼接后的字符串）\n",
    "        :return: 处理后的文本列表\n",
    "        \"\"\"\n",
    "        text_list = []  # 存放解码后的文本\n",
    "        for indices in indices_list:\n",
    "            text = []  # 存放当前句子的单词\n",
    "            for index in indices:\n",
    "                word = self.idx2word.get(index, \"[UNK]\")  # 通过索引获取单词，未知索引替换为 [UNK]\n",
    "\n",
    "                # 按参数决定是否移除特殊 token\n",
    "                if remove_bos and word == \"[BOS]\":\n",
    "                    continue\n",
    "                if remove_eos and word == \"[EOS]\":\n",
    "                    break  # [EOS] 之后的内容无需处理，直接跳出循环\n",
    "                if remove_pad and word == \"[PAD]\":\n",
    "                    break  # [PAD] 之后的内容无需处理，直接跳出循环\n",
    "\n",
    "                text.append(word)  # 添加单词到当前句子\n",
    "\n",
    "            # 根据 split 参数决定返回格式：\n",
    "            # - 若 split=False，则返回完整的句子字符串（单词以空格拼接）\n",
    "            # - 若 split=True，则返回单词列表\n",
    "            text_list.append(\" \".join(text) if not split else text)\n",
    "\n",
    "        return text_list  # 返回解码后的文本列表\n",
    "\n",
    "# 创建 Tokenizer 实例\n",
    "tokenizer = Tokenizer(word2idx=word2idx, idx2word=idx2word)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.502704Z",
     "iopub.status.busy": "2025-02-16T14:52:11.502387Z",
     "iopub.status.idle": "2025-02-16T14:52:11.512058Z",
     "shell.execute_reply": "2025-02-16T14:52:11.511367Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.502678Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.770352Z",
     "start_time": "2025-02-17T03:03:37.769351Z"
    }
   },
   "source": [
    "# 例子：准备输入数据，模拟分词后的文本列表\n",
    "raw_text = [\n",
    "    \"hello world\".split(),\n",
    "    \"tokenize text datas with batch\".split(),\n",
    "    \"this is a test\".split()\n",
    "]\n",
    "\n",
    "# 对输入文本进行编码\n",
    "indices = tokenizer.encode(raw_text, padding_first=False, add_bos=True, add_eos=True)\n",
    "\n",
    "# 对编码结果进行解码\n",
    "decode_text = tokenizer.decode(indices.tolist(), remove_bos=False, remove_eos=False, remove_pad=False)\n",
    "\n",
    "# 打印原始文本\n",
    "print(\"raw text\")\n",
    "for raw in raw_text:\n",
    "    print(raw)\n",
    "\n",
    "# 打印编码后的索引序列\n",
    "print(\"indices\")\n",
    "for index in indices:\n",
    "    print(index)\n",
    "\n",
    "# 打印解码后的文本\n",
    "print(\"decode text\")\n",
    "for decode in decode_text:\n",
    "    print(decode)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.513360Z",
     "iopub.status.busy": "2025-02-16T14:52:11.513066Z",
     "iopub.status.idle": "2025-02-16T14:52:11.517445Z",
     "shell.execute_reply": "2025-02-16T14:52:11.516638Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.513334Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.771352Z",
     "start_time": "2025-02-17T03:03:37.770352Z"
    }
   },
   "source": [
    "# 例子：检查 train_ds 数据集中源语言 (i) 和目标语言 (j) 句子的长度\n",
    "for i,j in train_ds:\n",
    "    print(len(i))\n",
    "    print(len(j))\n",
    "    break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftcEnnKxN4yB"
   },
   "source": [
    "### Transformer Batch Sampler\n",
    "\n",
    "> Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens\n",
    "句子按照序列长度差不多的分到一个批次。 每个训练批次包含一组句子对，其中包含大约 25000 个源标记和 25000 个目标标记"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.518973Z",
     "iopub.status.busy": "2025-02-16T14:52:11.518698Z",
     "iopub.status.idle": "2025-02-16T14:52:11.527158Z",
     "shell.execute_reply": "2025-02-16T14:52:11.526454Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.518947Z"
    },
    "id": "qP8nznIuN4yB"
   },
   "source": [
    "# 动态划分batch\n",
    "class SampleInfo:  # 下面的 info 对象\n",
    "    def __init__(self, i, lens):\n",
    "        \"\"\"\n",
    "        记录文本对的序号和长度信息\n",
    "        输入：\n",
    "            - i (int): 文本对的序号。\n",
    "            - lens (list): 文本对源语言和目标语言的长度\n",
    "        \"\"\"\n",
    "        self.i = i  # 记录当前文本对的序号\n",
    "        # 计算当前样本的最大长度，+1 是为了考虑填补在文本前后的特殊词元\n",
    "        self.max_len = max(lens[0], lens[1]) + 1\n",
    "        # 计算源语言文本的长度（包含特殊词元）\n",
    "        self.src_len = lens[0] + 1\n",
    "        # 计算目标语言文本的长度（包含特殊词元）\n",
    "        self.trg_len = lens[1] + 1\n",
    "\n",
    "# 一个批量生成器，根据词元数目的限制来控制批量的大小。\n",
    "# 它会根据传入的样本信息，在不超过设定大小的情况下，逐步构建批量。\n",
    "class TokenBatchCreator:\n",
    "    def __init__(self, batch_size):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "        batch_size (int): 用于限制批量的大小。\n",
    "        \n",
    "        功能:\n",
    "        - 初始化一个空的批量列表 `_batch`，用于存储当前批次的样本。\n",
    "        - 设定 `max_len` 初始值为 -1，用于记录当前批量中最长样本的长度。\n",
    "        - 存储传入的 `batch_size`，用于限制当前批次的最大 token 数量。\n",
    "        \"\"\"\n",
    "        self._batch = []  # 当前批次存储的样本列表\n",
    "        self.max_len = -1  # 当前批次最大样本长度，初始值设为 -1\n",
    "        self._batch_size = batch_size  # 批量大小的限制，例如 4096\n",
    "\n",
    "    def append(self, info: SampleInfo):\n",
    "        \"\"\"\n",
    "        向当前批量中添加一个样本，并控制批量大小。\n",
    "        \n",
    "        参数:\n",
    "        - info (SampleInfo): 需要添加的样本信息对象，包含文本对的长度信息。\n",
    "\n",
    "        逻辑:\n",
    "        1. 计算当前样本的长度 `cur_len`。\n",
    "        2. 更新当前批量的最大样本长度 `max_len`。\n",
    "        3. 检查如果当前批量的 token 数量（`max_len * 样本数`）超过 `_batch_size`：\n",
    "           - 返回当前批量（表示该批量已经满了）。\n",
    "           - 清空 `_batch` 并将当前样本作为新批量的第一个样本。\n",
    "           - `max_len` 设为当前样本的长度。\n",
    "        4. 否则，将该样本添加到当前批量，并更新 `max_len`。\n",
    "\n",
    "        返回:\n",
    "        - 如果批量已满，则返回当前批量，并开启新的批量存储新样本。\n",
    "        - 如果批量未满，则返回 `None`。\n",
    "        \"\"\"\n",
    "        cur_len = info.max_len  # 当前样本的最大长度\n",
    "        max_len = max(self.max_len, cur_len)  # 更新当前批量的最大样本长度\n",
    "\n",
    "        # 计算当前批量的 token 数量（最大长度 * 样本数），如果超过 batch_size，则返回当前批量\n",
    "        if max_len * (len(self._batch) + 1) > self._batch_size:\n",
    "            self._batch, result = [], self._batch  # 保存当前批量，并清空 `_batch`\n",
    "            self._batch.append(info)  # 将当前样本作为新批量的第一个样本\n",
    "            self.max_len = cur_len  # 设定新批量的最大样本长度\n",
    "            return result  # 返回已满的批量\n",
    "        else:\n",
    "            self.max_len = max_len  # 更新最大样本长度\n",
    "            self._batch.append(info)  # 添加样本到当前批量\n",
    "            return None  # 当前批量未满，返回 None\n",
    "\n",
    "    @property # @property装饰器可以让方法像属性一样被调用\n",
    "    def batch(self):\n",
    "        \"\"\"返回当前批量中存储的样本列表\"\"\"\n",
    "        return self._batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.528720Z",
     "iopub.status.busy": "2025-02-16T14:52:11.528289Z",
     "iopub.status.idle": "2025-02-16T14:52:11.801760Z",
     "shell.execute_reply": "2025-02-16T14:52:11.800843Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.528694Z"
    },
    "id": "_Vtc0gXEN4yB",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.775353Z",
     "start_time": "2025-02-17T03:03:37.774351Z"
    }
   },
   "source": [
    "from torch.utils.data import BatchSampler\n",
    "\n",
    "class TransformerBatchSampler(BatchSampler): # 继承自 BatchSampler\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch_size,\n",
    "                 shuffle_batch=False,\n",
    "                 clip_last_batch=False,\n",
    "                 seed=0):\n",
    "        \"\"\"\n",
    "        Transformer 训练的批量采样器，用于根据 token 数量进行批量划分。\n",
    "        \n",
    "        输入:\n",
    "            - dataset: 数据集，包含多个文本对，每个文本对包含源语言和目标语言的文本。\n",
    "            - batch_size: 设定的批量大小（以 token 计算）。\n",
    "            用于随机的参数：\n",
    "            - shuffle_batch: 是否对生成的批次进行洗牌（即打乱批次的顺序）。\n",
    "            - clip_last_batch: 是否丢弃最后一个未达到 batch_size 的批次。\n",
    "            - seed: 随机种子，用于确保每次运行的洗牌结果相同。\n",
    "        \n",
    "        逻辑:\n",
    "        1. 记录传入的参数，初始化随机数生成器（用于洗牌）。\n",
    "        2. 遍历数据集中的每个样本，计算其源语言和目标语言的 token 长度。\n",
    "        3. 创建 `SampleInfo` 对象，存储每个样本的索引及长度信息，并存入 `_sample_infos` 列表。\n",
    "        \"\"\"\n",
    "\n",
    "        self._dataset = dataset  # 数据集:train_ds\n",
    "        self._batch_size = batch_size  # 设定的最大批量大小（以 token 数量计算） 假设为4096\n",
    "        self._shuffle_batch = shuffle_batch  # 是否对批次顺序进行随机洗牌\n",
    "        self._clip_last_batch = clip_last_batch  # 是否裁剪最后不足 batch_size 的批次\n",
    "        self._seed = seed  # 随机种子，确保每次打乱的顺序一致\n",
    "\n",
    "        # 初始化 NumPy 随机数生成器\n",
    "        self._random = np.random\n",
    "        self._random.seed(seed)\n",
    "\n",
    "        # 存储所有样本的信息\n",
    "        self._sample_infos = []  \n",
    "        \n",
    "        # 遍历数据集，计算每个样本的源语言和目标语言长度，并存入 SampleInfo 对象\n",
    "        for i, data in enumerate(self._dataset): # enumerate() 函数用于遍历数据集，返回索引和数据\n",
    "            lens = [len(data[0]), len(data[1])]  # 计算源语言和目标语言的长度\n",
    "            self._sample_infos.append(SampleInfo(i, lens))  # 记录样本的索引和长度信息\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        生成批量索引，用于训练数据的采样。\n",
    "        \n",
    "        逻辑:\n",
    "        1. 先按样本的源语言长度排序，若相同再按目标语言长度排序，使得相似长度的样本聚合在一起，减少填充（padding）。\n",
    "        2. 依次将样本放入 `TokenBatchCreator` 进行分批，保证每批的 token 数量不超过 batch_size。\n",
    "        3. 若 `clip_last_batch` 为 False，则保留最后不足 batch_size 的批次。\n",
    "        4. 若 `shuffle_batch` 为 True，则打乱所有批次的顺序。\n",
    "        5. 逐个返回批次，每个批次包含多个样本的索引，表示数据集中哪些样本属于该批次。\n",
    "\n",
    "        返回:\n",
    "        - 迭代器，每次返回一个批次的样本索引列表（如 [0,1,2,...,83]）。\n",
    "        \"\"\"\n",
    "\n",
    "        # 按照 (src_len, trg_len) 进行排序，使得相似长度的样本排列在一起，减少填充需求\n",
    "        infos = sorted(self._sample_infos, key=lambda x: (x.src_len, x.trg_len))\n",
    "\n",
    "        batch_infos = []  # 存储所有批次的信息\n",
    "        batch_creator = TokenBatchCreator(self._batch_size)  # 初始化批量生成器\n",
    "\n",
    "        # 遍历排序后的样本信息，并放入批量生成器\n",
    "        for info in infos:\n",
    "            batch = batch_creator.append(info)  # 添加样本到当前批次\n",
    "            if batch is not None:\n",
    "                batch_infos.append(batch)  # 当前批次已满，存储该批次\n",
    "\n",
    "        # 处理最后一个批次\n",
    "        if not self._clip_last_batch and len(batch_creator.batch) != 0:\n",
    "            batch_infos.append(batch_creator.batch)  # 将剩余样本作为最后一个批次\n",
    "\n",
    "        # 是否对批次进行洗牌（即打乱批次顺序，而不是打乱样本顺序）\n",
    "        if self._shuffle_batch:\n",
    "            self._random.shuffle(batch_infos)\n",
    "\n",
    "        self.batch_number = len(batch_infos)  # 计算总批次数量\n",
    "\n",
    "        # 依次返回每个批次的样本索引\n",
    "        for batch in batch_infos:\n",
    "            batch_indices = [info.i for info in batch]  # 提取该批次所有样本在数据集中的索引\n",
    "            yield batch_indices  # 生成该批次的索引列表\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回批次数量（即数据集被划分成了多少个批次）。\n",
    "        \n",
    "        逻辑:\n",
    "        - 若 `batch_number` 已计算，则直接返回。\n",
    "        - 否则，计算数据集中样本总数与 batch_size 的比值，得到批次数量（一般情况下不会走到这个逻辑）。\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"batch_number\"):\n",
    "            return self.batch_number\n",
    "\n",
    "        # 计算批次数量（一般情况下不会用到此计算逻辑）\n",
    "        batch_number = (len(self._dataset) + self._batch_size) // self._batch_size\n",
    "        return batch_number\n",
    "\n",
    "# 长度*batch_number>4096的时候，就会返回上一个batch，然后新的样本加入新的batch,具体要看TokenBatchCreator的44行代码\n",
    "sampler = TransformerBatchSampler(train_ds, batch_size=4096, shuffle_batch=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.803099Z",
     "iopub.status.busy": "2025-02-16T14:52:11.802733Z",
     "iopub.status.idle": "2025-02-16T14:52:11.848190Z",
     "shell.execute_reply": "2025-02-16T14:52:11.847245Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.803070Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.776353Z",
     "start_time": "2025-02-17T03:03:37.775353Z"
    }
   },
   "source": [
    "# 例子：打印一个批次的样本索引\n",
    "for idx, batch in enumerate(sampler):\n",
    "    print(\"第{}批量的数据中含有文本对是：{}，数量为：{}\".format(idx, batch, len(batch)))\n",
    "    if idx >= 3:\n",
    "        break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.849837Z",
     "iopub.status.busy": "2025-02-16T14:52:11.849344Z",
     "iopub.status.idle": "2025-02-16T14:52:11.856539Z",
     "shell.execute_reply": "2025-02-16T14:52:11.855845Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.849792Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.778353Z",
     "start_time": "2025-02-17T03:03:37.777354Z"
    }
   },
   "source": [
    "len(sampler)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Olkaw4JNN4yC"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.857945Z",
     "iopub.status.busy": "2025-02-16T14:52:11.857451Z",
     "iopub.status.idle": "2025-02-16T14:52:11.865455Z",
     "shell.execute_reply": "2025-02-16T14:52:11.864590Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.857915Z"
    },
    "id": "rvvuNJIzN4yC",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.778353Z",
     "start_time": "2025-02-17T03:03:37.778353Z"
    }
   },
   "source": [
    "def collate_fct(batch, tokenizer):\n",
    "    \"\"\"\n",
    "    处理一个批次的数据，将其转化为模型输入所需要的格式。\n",
    "    \n",
    "    输入：\n",
    "        - batch: 由多个样本（文本对）组成的批次。每个样本是一个包含源语言和目标语言的元组 `(src_text, trg_text)`。\n",
    "        - tokenizer: 用于对源语言和目标语言文本进行编码的分词器。\n",
    "\n",
    "    输出：\n",
    "        返回一个字典，包含处理后的 encoder 和 decoder 的输入及标签。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 将批次中的每个文本对的源语言和目标语言分开，按空格分词\n",
    "    src_words = [pair[0].split() for pair in batch]  # 列表中的每个元素是源语言分词后的单词列表\n",
    "    trg_words = [pair[1].split() for pair in batch]  # 列表中的每个元素是目标语言分词后的单词列表\n",
    "\n",
    "    # 对源语言进行编码，添加 [BOS] (开始标记)、[EOS] (结束标记) 并进行填充\n",
    "    encoder_inputs, encoder_inputs_mask = tokenizer.encode(\n",
    "        src_words, \n",
    "        padding_first=False,  # 填充在序列末尾（前填充设为 False）\n",
    "        add_bos=True,         # 添加开始标记 [BOS]\n",
    "        add_eos=True,         # 添加结束标记 [EOS]\n",
    "        return_mask=True      # 返回对应的 mask（用于标识有效单词位置）\n",
    "    )\n",
    "\n",
    "    # 对目标语言进行编码，添加 [BOS] 标记，未添加 [EOS]，不返回 mask\n",
    "    decoder_inputs = tokenizer.encode(\n",
    "        trg_words, \n",
    "        padding_first=False,  # 填充在序列末尾\n",
    "        add_bos=True,         # 添加开始标记 [BOS]\n",
    "        add_eos=False,        # 不添加结束标记 [EOS]\n",
    "        return_mask=False     # 不需要返回 mask\n",
    "    )\n",
    "\n",
    "    # 对目标语言进行编码，添加 [EOS]，未添加 [BOS]，并返回 mask\n",
    "    decoder_labels, decoder_labels_mask = tokenizer.encode(\n",
    "        trg_words, \n",
    "        padding_first=False,  # 填充在序列末尾\n",
    "        add_bos=False,        # 不添加开始标记 [BOS]\n",
    "        add_eos=True,         # 添加结束标记 [EOS]\n",
    "        return_mask=True      # 返回 mask\n",
    "    )\n",
    "\n",
    "    # 将处理后的结果转移到设备（如 GPU）上\n",
    "    return {\n",
    "        \"encoder_inputs\": encoder_inputs.to(device=device),                # 源语言编码\n",
    "        \"encoder_inputs_mask\": encoder_inputs_mask.to(device=device),      # 源语言的 mask\n",
    "        \"decoder_inputs\": decoder_inputs.to(device=device),                # 目标语言编码（不包含 [EOS]）\n",
    "        \"decoder_labels\": decoder_labels.to(device=device),                # 目标语言标签（包含 [EOS]）\n",
    "        \"decoder_labels_mask\": decoder_labels_mask.to(device=device),      # 目标语言标签的 mask\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.867143Z",
     "iopub.status.busy": "2025-02-16T14:52:11.866688Z",
     "iopub.status.idle": "2025-02-16T14:52:11.966540Z",
     "shell.execute_reply": "2025-02-16T14:52:11.965811Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.867097Z"
    },
    "id": "5p79gPo5N4yC",
    "outputId": "2b1d43ae-a2ed-4333-8274-50c62c2fc6c9",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.780352Z",
     "start_time": "2025-02-17T03:03:37.779354Z"
    }
   },
   "source": [
    "from functools import partial  # 固定collate_fct的tokenizer参数\n",
    "\n",
    "# 创建一个 TransformerBatchSampler 实例，用于控制批量采样的行为。\n",
    "# batch_size: 批量大小，指定每个批次中的样本数（这里设为128）。\n",
    "# shuffle_batch: 是否打乱批次，设为True表示打乱批次顺序。\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "# 可以调大batch_size,来看最终的bleu，如果GPU内存不够，可以减小batch_size\n",
    "sampler = TransformerBatchSampler(train_ds, batch_size=256, shuffle_batch=False)\n",
    "\n",
    "# 使用 DataLoader 创建一个数据加载器，管理数据的批量处理。\n",
    "# - train_ds: 数据集对象，通常是训练集。\n",
    "# - batch_sampler: 使用之前创建的 TransformerBatchSampler，来控制批量的生成。\n",
    "# - collate_fn: 批量处理函数，这里使用 partial 固定了 collate_fct 函数的 tokenizer 参数。\n",
    "# 这样可以避免每次都手动传递 tokenizer，而是将其固定为一个参数。\n",
    "sample_dl = DataLoader(train_ds, batch_sampler=sampler, collate_fn=partial(collate_fct, tokenizer=tokenizer))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:11.967696Z",
     "iopub.status.busy": "2025-02-16T14:52:11.967364Z",
     "iopub.status.idle": "2025-02-16T14:52:12.011087Z",
     "shell.execute_reply": "2025-02-16T14:52:12.010307Z",
     "shell.execute_reply.started": "2025-02-16T14:52:11.967669Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.781354Z",
     "start_time": "2025-02-17T03:03:37.781354Z"
    }
   },
   "source": [
    "# 例子：查看一个批次的索引列表\n",
    "for i in sampler: # 因为TransformerBatchSampler中实现了__iter__方法，所以可以直接用for循环来遍历\n",
    "    print(f\"批次的索引列表为:{i}\")\n",
    "    print(f\"批次中的样本数量为:{len(i)}\")\n",
    "    break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.012203Z",
     "iopub.status.busy": "2025-02-16T14:52:12.011894Z",
     "iopub.status.idle": "2025-02-16T14:52:12.279617Z",
     "shell.execute_reply": "2025-02-16T14:52:12.278815Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.012177Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.783352Z",
     "start_time": "2025-02-17T03:03:37.782351Z"
    }
   },
   "source": [
    "# 例子：使用 DataLoader 迭代批次\n",
    "for batch in sample_dl:  # 外层循环遍历每个批次\n",
    "    for key, value in batch.items():  # 内层循环遍历每个批次字典中的键值对\n",
    "        print(key)   # 打印批次中的键（例如 \"encoder_inputs\", \"decoder_inputs\" 等）\n",
    "        print(value) # 打印批次中的值，通常是 Tensor 或 numpy 数组\n",
    "    break  # 只打印第一个批次"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AadtoM3N4yC"
   },
   "source": [
    "## 定义模型\n",
    "\n",
    "- Transformer模型由Embedding、Transformer-Block组成\n",
    "- Embedding包括：\n",
    "    - WordEmbedding\n",
    "    - PositionEmbedding\n",
    "- Transformer-Block包括：\n",
    "    - Self-Attention\n",
    "    - Cross-Attention\n",
    "    - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTl-sSJmN4yD"
   },
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.285512Z",
     "iopub.status.busy": "2025-02-16T14:52:12.285126Z",
     "iopub.status.idle": "2025-02-16T14:52:12.291579Z",
     "shell.execute_reply": "2025-02-16T14:52:12.290776Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.285480Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.784352Z",
     "start_time": "2025-02-17T03:03:37.784352Z"
    }
   },
   "source": [
    "torch.arange(0, 128).unsqueeze(1).shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.292940Z",
     "iopub.status.busy": "2025-02-16T14:52:12.292523Z",
     "iopub.status.idle": "2025-02-16T14:52:12.532816Z",
     "shell.execute_reply": "2025-02-16T14:52:12.532074Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.292914Z"
    },
    "id": "y66CxrsBN4yD",
    "outputId": "c703025c-afc5-4012-d8e3-0e9d4ad253ec",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.786351Z",
     "start_time": "2025-02-17T03:03:37.785349Z"
    }
   },
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 从配置中获取超参数\n",
    "        self.vocab_size = config[\"vocab_size\"]  # 词汇表的大小\n",
    "        self.hidden_size = config[\"d_model\"]  # 词向量的维度\n",
    "        self.pad_idx = config[\"pad_idx\"]  # padding的索引\n",
    "        dropout_rate = config[\"dropout\"]  # dropout的比率\n",
    "        self.max_length = config[\"max_length\"]  # 序列的最大长度\n",
    "\n",
    "        # 初始化词嵌入层，padding_idx表示pad的词向量全为0\n",
    "        self.word_embedding = nn.Embedding(self.vocab_size, self.hidden_size, padding_idx=self.pad_idx)\n",
    "        # 初始化位置嵌入层，_weight表示直接给位置编码的权重\n",
    "        self.pos_embedding = nn.Embedding(\n",
    "            self.max_length, \n",
    "            self.hidden_size,\n",
    "            #位置编码，权重通过get_positional_encoding函数计算得到\n",
    "            _weight=self.get_positional_encoding(self.max_length, self.hidden_size), \n",
    "        )\n",
    "        self.pos_embedding.weight.requires_grad_(False)  # 位置编码的权重不需要更新\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # 随机失活层\n",
    "\n",
    "    def get_word_embedding_weights(self):\n",
    "        # 获取词嵌入层的权重\n",
    "        return self.word_embedding.weight\n",
    "\n",
    "    @classmethod\n",
    "    def get_positional_encoding(self, max_length, hidden_size):\n",
    "        \"\"\"\n",
    "        计算位置编码：通过正弦和余弦函数生成位置编码矩阵\n",
    "        输入：\n",
    "            max_length: 最大序列长度\n",
    "            hidden_size: 词向量维度\n",
    "        输出：\n",
    "            pe: 位置编码矩阵\n",
    "        \"\"\"\n",
    "        # 初始化一个零矩阵，大小为(max_length, hidden_size)\n",
    "        pe = torch.zeros(max_length, hidden_size)\n",
    "        \n",
    "        # position是一个大小为(max_length, 1)的张量，包含从0到max_length-1的整数\n",
    "        position = torch.arange(0, max_length).unsqueeze(1)  # 位置信息，从0到max_length-1\n",
    "        # div_term计算每个位置的缩放因子，目的是通过对数空间进行归一化\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, hidden_size, 2)\n",
    "            * -(torch.log(torch.Tensor([10000.0])) / hidden_size)\n",
    "        )\n",
    "        # 对位置编码矩阵进行填充\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数列填充sin值\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数列填充cos值\n",
    "        \n",
    "        return pe\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        前向传播：将输入的id转换为词向量和位置编码的加和\n",
    "        输入：\n",
    "            input_ids: 形状为[batch_size, seq_len]的张量，包含输入序列的id\n",
    "        输出：\n",
    "            embeds: 形状为[batch_size, seq_len, hidden_size]的张量，词向量和位置编码的加和\n",
    "        \"\"\"\n",
    "        seq_len = input_ids.shape[1]  # 获取序列的长度\n",
    "        # 检查输入序列的长度是否超出最大长度\n",
    "        assert (\n",
    "            seq_len <= self.max_length\n",
    "        ), f\"input sequence length should no more than {self.max_length} but got {seq_len}\"\n",
    "\n",
    "        # 生成位置id\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        # print(position_ids)  # 调试输出位置id\n",
    "\n",
    "        # 词嵌入：将input_ids转换为词向量\n",
    "        word_embeds = self.word_embedding(input_ids)\n",
    "        # 位置嵌入：将position_ids转换为位置编码\n",
    "        pos_embeds = self.pos_embedding(position_ids)\n",
    "        # 将词向量和位置编码相加\n",
    "        embeds = word_embeds + pos_embeds\n",
    "        # 使用dropout进行随机失活\n",
    "        embeds = self.dropout(embeds)\n",
    "\n",
    "        return embeds\n",
    "\n",
    "\n",
    "def plot_position_embedding(position_embedding):\n",
    "    \"\"\"\n",
    "    绘制位置编码矩阵\n",
    "    输入：\n",
    "        position_embedding: 位置编码矩阵\n",
    "    输出：\n",
    "        无\n",
    "    \"\"\"\n",
    "    plt.pcolormesh(position_embedding)  # 绘制位置编码矩阵\n",
    "    plt.xlabel('Depth')  # x轴为深度\n",
    "    plt.ylabel('Position')  # y轴为位置\n",
    "    plt.colorbar()  # 添加颜色条，表示编码值的范围\n",
    "    plt.show()  # 显示图像\n",
    "\n",
    "# 获取64个位置、128维词向量的位置编码矩阵并绘制\n",
    "position_embedding = TransformerEmbedding.get_positional_encoding(64, 128)\n",
    "plot_position_embedding(position_embedding)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.534113Z",
     "iopub.status.busy": "2025-02-16T14:52:12.533747Z",
     "iopub.status.idle": "2025-02-16T14:52:12.538984Z",
     "shell.execute_reply": "2025-02-16T14:52:12.538372Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.534078Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.787351Z",
     "start_time": "2025-02-17T03:03:37.786351Z"
    }
   },
   "source": [
    "# 例子：为了理解指数对数变换\n",
    "import math\n",
    "def positional_encoding_weights(emb_size):\n",
    "    # 计算底数和指数\n",
    "    base = math.log(10000)\n",
    "    exponent = -5 / emb_size\n",
    "    \n",
    "    # 应用指数函数\n",
    "    weight = math.exp(exponent*base) \n",
    "    \n",
    "    return weight\n",
    "\n",
    "emb_size = 10  # 举例，可以替换成你需要的大小\n",
    "weight = positional_encoding_weights(emb_size)\n",
    "print(weight)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.540323Z",
     "iopub.status.busy": "2025-02-16T14:52:12.539987Z",
     "iopub.status.idle": "2025-02-16T14:52:12.544945Z",
     "shell.execute_reply": "2025-02-16T14:52:12.544276Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.540295Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.788351Z",
     "start_time": "2025-02-17T03:03:37.788351Z"
    }
   },
   "source": [
    "1/10000**(1/2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.546010Z",
     "iopub.status.busy": "2025-02-16T14:52:12.545710Z",
     "iopub.status.idle": "2025-02-16T14:52:12.553895Z",
     "shell.execute_reply": "2025-02-16T14:52:12.553259Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.545984Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.790351Z",
     "start_time": "2025-02-17T03:03:37.789352Z"
    }
   },
   "source": [
    "#例子：随机input，调用TransformerEmbedding\n",
    "config={\n",
    "    \"vocab_size\": 100,\n",
    "    \"d_model\": 128,\n",
    "    \"pad_idx\": 0,\n",
    "    \"max_length\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "input_ids = torch.randint(0, 100, (2, 50))\n",
    "embeds = TransformerEmbedding(config)(input_ids)\n",
    "embeds.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXavO0SNN4yD"
   },
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZL4mu3pN4yD"
   },
   "source": [
    "#### scaled-dot-product-attention"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 970
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.555268Z",
     "iopub.status.busy": "2025-02-16T14:52:12.554754Z",
     "iopub.status.idle": "2025-02-16T14:52:12.568538Z",
     "shell.execute_reply": "2025-02-16T14:52:12.567839Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.555241Z"
    },
    "id": "U9QwdVQYN4yD",
    "outputId": "b6ae9ac8-c71c-4a7b-fd4b-9c15e92dba2c"
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# 定义 AttentionOutput 数据类，用于存储多头注意力的输出\n",
    "Tensor = torch.Tensor\n",
    "@dataclass\n",
    "class AttentionOutput:\n",
    "    hidden_states: Tensor  # 注意力层的最终输出\n",
    "    attn_scores: Tensor    # 计算得到的注意力权重\n",
    "\n",
    "# 多头注意力层\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 获取配置中的超参数\n",
    "        self.hidden_size = config[\"d_model\"]  # 隐藏层维度（例如 512）\n",
    "        self.num_heads = config[\"num_heads\"]  # 头的数量（例如 8）\n",
    "        \n",
    "        # 断言，确保 hidden_size 可以被 num_heads 整除\n",
    "        assert (\n",
    "            self.hidden_size % self.num_heads == 0\n",
    "        ), \"Hidden size must be divisible by num_heads but got {} and {}\".format(\n",
    "            self.hidden_size, self.num_heads\n",
    "        )\n",
    "\n",
    "        # 计算每个头的维度（head_dim = hidden_size / num_heads）\n",
    "        self.head_dim = self.hidden_size // self.num_heads  \n",
    "\n",
    "        # 定义线性变换层，将输入的 query、key、value 投影到 hidden_size 维度\n",
    "        self.Wq = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  # Q 投影层\n",
    "        self.Wk = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  # K 投影层\n",
    "        self.Wv = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  # V 投影层\n",
    "        self.Wo = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  # 输出投影层\n",
    "\n",
    "    def _split_heads(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        将输入张量 x 拆分为多个头的形式\n",
    "        输入: x -> 形状为 [batch_size, seq_len, hidden_size]\n",
    "        输出: num_heads * head_dim = hidden_size -> [batch_size, seq_len, num_heads, head_dim] \n",
    "        \"\"\"\n",
    "        bs, seq_len, _ = x.shape  # 获取 batch_size 和序列长度\n",
    "        x = x.view(bs, seq_len, self.num_heads, self.head_dim)  # 变形，使得每个头拥有 head_dim 维度\n",
    "        return x.permute(0, 2, 1, 3)  # 交换维度，调整为 [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "    def _merge_heads(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        将多个头的注意力结果合并回一个张量\n",
    "        输入: [batch_size, num_heads, seq_len, head_dim]\n",
    "        输出: [batch_size, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        bs, _, seq_len, _ = x.shape  # 获取 batch_size 和序列长度\n",
    "        return x.permute(0, 2, 1, 3).reshape(bs, seq_len, self.hidden_size)  # 重新排列维度并恢复原形状\n",
    "\n",
    "    def forward(self, querys, keys, values, attn_mask=None) -> AttentionOutput: # -> 表示返回值类型\n",
    "        \"\"\"\n",
    "        前向传播计算注意力机制\n",
    "        输入:\n",
    "            querys: [batch_size, seq_len, hidden_size]\n",
    "            keys:   [batch_size, seq_len, hidden_size]\n",
    "            values: [batch_size, seq_len, hidden_size]\n",
    "            attn_mask: [batch_size, 1, seq_len, seq_len] (可选)\n",
    "        输出:\n",
    "            AttentionOutput(hidden_states=[batch_size, seq_len, hidden_size], attn_scores=[batch_size, num_heads, seq_len, seq_len])\n",
    "        \"\"\"\n",
    "\n",
    "        # 线性变换获取 Q、K、V，并拆分为多个头\n",
    "        querys = self._split_heads(self.Wq(querys))  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        keys = self._split_heads(self.Wk(keys))      # [batch_size, num_heads, seq_len, head_dim]\n",
    "        values = self._split_heads(self.Wv(values))  # [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # 计算 Q 和 K 之间的点积注意力分数，两个张量相乘只会对最后两个维度进行计算 [seq_len, head_dim] x [head_dim, seq_len] = [seq_len, seq_len]\n",
    "        qk_logits = torch.matmul(querys, keys.mT)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # 如果提供了注意力 mask，则应用它\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask[:, :, : querys.shape[-2], : keys.shape[-2]]  # 调整 mask 尺寸\n",
    "            qk_logits += attn_mask * -1e9  # 将需要 mask 的部分赋值为负无穷\n",
    "\n",
    "        # 计算注意力权重（Softmax 归一化）\n",
    "        # dim等于那个维度，就是把那个维度给消除了，比如说shape=(2,3,4),如果dim=0,最后的结果的shape=(3,4),如果dim=1,最后的结果的shape=(2,4)，如果dim=2的话，最后的结果的shape=(2,3)\n",
    "        attn_scores = F.softmax(qk_logits / (self.head_dim**0.5), dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # 计算注意力加权求和 [seq_len, seq_len] x [seq_len, head_dim] = [seq_len, head_dim]\n",
    "        embeds = torch.matmul(attn_scores, values)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # 重新合并多头的输出 [batch_size, num_heads, seq_len, hidden_dim] -> [batch_size, seq_len, hidden_size]\n",
    "        embeds = self.Wo(self._merge_heads(embeds))  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # 返回注意力输出\n",
    "        return AttentionOutput(hidden_states=embeds, attn_scores=attn_scores)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.569641Z",
     "iopub.status.busy": "2025-02-16T14:52:12.569325Z",
     "iopub.status.idle": "2025-02-16T14:52:12.576908Z",
     "shell.execute_reply": "2025-02-16T14:52:12.576228Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.569615Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.794353Z",
     "start_time": "2025-02-17T03:03:37.793351Z"
    }
   },
   "source": [
    "# 例子：测试 MultiHeadAttention\n",
    "mha = MultiHeadAttention({\"num_heads\": 2, \"d_model\": 2})  # 创建多头注意力实例\n",
    "query = torch.randn(2, 3, 2)  # [batch_size, seq_len, hidden_size]\n",
    "query /= query.norm(dim=-1, keepdim=True)  # 归一化\n",
    "key_value = torch.randn(2, 4, 2)  # [batch_size, seq_len, hidden_size]\n",
    "print(f'key_value.shape {key_value.shape}')  # 打印 key_value 的形状\n",
    "\n",
    "# 计算注意力\n",
    "outputs = mha(query, key_value, key_value)  \n",
    "\n",
    "# 打印最终输出的形状\n",
    "print(outputs.hidden_states.shape)  # 期待输出: [2, 3, 2]\n",
    "print(outputs.attn_scores.shape)  # 期待输出: [2, 2, 3, 4] (batch_size, num_heads, seq_len, seq_len)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.577983Z",
     "iopub.status.busy": "2025-02-16T14:52:12.577692Z",
     "iopub.status.idle": "2025-02-16T14:52:12.583279Z",
     "shell.execute_reply": "2025-02-16T14:52:12.582620Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.577958Z"
    },
    "ExecuteTime": {
     "start_time": "2025-02-17T03:03:37.794353Z"
    }
   },
   "source": [
    "# 例子：生成一个随机的 2x3 张量，模拟输入数据\n",
    "x = torch.randn(2, 3)\n",
    "\n",
    "# 在最后一个维度 (-1 维，即列方向) 进行 softmax 归一化\n",
    "# softmax 作用：将输入张量转换为概率分布，使得每一行的值之和为 1\n",
    "x_softmax = F.softmax(x, dim=-1)\n",
    "\n",
    "# 打印 softmax 计算后的张量\n",
    "print(x_softmax)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:12.584347Z",
     "iopub.status.busy": "2025-02-16T14:52:12.584043Z",
     "iopub.status.idle": "2025-02-16T14:52:13.058818Z",
     "shell.execute_reply": "2025-02-16T14:52:13.058068Z",
     "shell.execute_reply.started": "2025-02-16T14:52:12.584321Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.795352Z",
     "start_time": "2025-02-17T03:03:37.795352Z"
    }
   },
   "source": [
    "# 例子：创建一个子图网格，其维度由 outputs.attn_scores.shape[:2] 决定\n",
    "# outputs.attn_scores.shape[:2] 代表 [batch_size, num_heads]\n",
    "# 例如，如果 outputs.attn_scores 形状为 (2, 2, 3, 4)，则创建 2x2 的子图网格\n",
    "fig, axis = plt.subplots(*outputs.attn_scores.shape[:2])\n",
    "\n",
    "# 遍历 batch 维度（i 代表 batch_size）\n",
    "for i in range(query.shape[0]):  # batch_size\n",
    "    # 遍历多头注意力维度（j 代表 num_heads）\n",
    "    for j in range(outputs.attn_scores.shape[1]):  # num_heads\n",
    "        # 在 axis[i, j] 位置绘制当前头的注意力分数热图\n",
    "        # outputs.attn_scores[i, j] 形状为 (seq_len, seq_len)\n",
    "        # detach().numpy() 将 PyTorch 张量转换为 NumPy 数组，便于可视化\n",
    "        axis[i, j].matshow(outputs.attn_scores[i, j].detach().numpy())\n",
    "\n",
    "        # 遍历行 (query 序列长度)\n",
    "        for x in range(outputs.attn_scores.shape[2]):  # seq_len (query)\n",
    "            # 遍历列 (key 序列长度)\n",
    "            for y in range(outputs.attn_scores.shape[3]):  # seq_len (key)\n",
    "                # 在热图上添加文字，显示每个 (x, y) 位置的注意力分数\n",
    "                # 格式化为两位小数，文本居中，并使用白色字体\n",
    "                axis[i, j].text(\n",
    "                    y, x, \n",
    "                    f\"{outputs.attn_scores[i, j, x, y]:.2f}\", \n",
    "                    ha=\"center\", va=\"center\", color=\"w\"\n",
    "                )\n",
    "\n",
    "# 设置图表标题\n",
    "fig.suptitle(\"multi head attention without mask\")\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:13.060141Z",
     "iopub.status.busy": "2025-02-16T14:52:13.059771Z",
     "iopub.status.idle": "2025-02-16T14:52:13.386084Z",
     "shell.execute_reply": "2025-02-16T14:52:13.385201Z",
     "shell.execute_reply.started": "2025-02-16T14:52:13.060112Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.796352Z",
     "start_time": "2025-02-17T03:03:37.796352Z"
    }
   },
   "source": [
    "# 例子：构造一个手动指定的 Mask 矩阵\n",
    "# 形状为 (1, 1, 3, 4)，用于屏蔽部分注意力权重\n",
    "# 其中 1 代表被屏蔽（会被赋予 -∞ 值，使其 softmax 后趋近于 0）\n",
    "mask = torch.Tensor([\n",
    "    [0, 0, 1, 1],  # 第 1 行：屏蔽后两个位置\n",
    "    [0, 0, 0, 1],  # 第 2 行：屏蔽最后一个位置\n",
    "    [0, 0, 0, 0]   # 第 3 行：不过滤任何位置\n",
    "]).reshape(1, 1, 3, 4)  # 形状调整为 (batch_size=1, num_heads=1, query_len=3, key_len=4)\n",
    "\n",
    "# 进行带 Mask 的多头注意力计算\n",
    "outputs_masked = mha(query, key_value, key_value, mask)\n",
    "\n",
    "# 创建子图网格，其维度由 outputs_masked.attn_scores.shape[:2] 决定\n",
    "# 对应 (batch_size, num_heads)\n",
    "fig, axis = plt.subplots(*outputs_masked.attn_scores.shape[:2])\n",
    "\n",
    "# 遍历 batch 维度（i 代表 batch_size）\n",
    "for i in range(query.shape[0]):  # batch_size\n",
    "    # 遍历多头注意力维度（j 代表 num_heads）\n",
    "    for j in range(outputs_masked.attn_scores.shape[1]):  # num_heads\n",
    "        # 绘制注意力分数热图（matshow 用于可视化矩阵）\n",
    "        axis[i, j].matshow(outputs_masked.attn_scores[i, j].detach().numpy())\n",
    "\n",
    "        # 遍历 query 维度（行数）\n",
    "        for x in range(outputs_masked.attn_scores.shape[2]):  # seq_len (query)\n",
    "            # 遍历 key 维度（列数）\n",
    "            for y in range(outputs_masked.attn_scores.shape[3]):  # seq_len (key)\n",
    "                # 在热图上叠加数值，格式化为两位小数\n",
    "                axis[i, j].text(\n",
    "                    y, x,\n",
    "                    f\"{outputs_masked.attn_scores[i, j, x, y]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\", color=\"w\"\n",
    "                )\n",
    "\n",
    "# 设置标题，标识该注意力计算使用了 Mask\n",
    "fig.suptitle(\"multi head attention with mask\")\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkzzrRWVN4yE"
   },
   "source": [
    "#### Transformer-Block"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:13.387618Z",
     "iopub.status.busy": "2025-02-16T14:52:13.387261Z",
     "iopub.status.idle": "2025-02-16T14:52:13.400473Z",
     "shell.execute_reply": "2025-02-16T14:52:13.399661Z",
     "shell.execute_reply.started": "2025-02-16T14:52:13.387589Z"
    },
    "id": "b16xpL47N4yE",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.798352Z",
     "start_time": "2025-02-17T03:03:37.798352Z"
    }
   },
   "source": [
    "# @dataclass 装饰器：为TransformerBlockOutput类自动生成 __init__()、__repr__()、__eq__() 等方法。\n",
    "@dataclass\n",
    "class TransformerBlockOutput:\n",
    "    # hidden_states: Tensor：表示当前块的输出（即隐藏状态），它包含了该块计算后的结果\n",
    "    hidden_states: Tensor\n",
    "    # self_attn_scores: Tensor：表示自注意力机制计算的注意力分数（self-attention scores）\n",
    "    self_attn_scores: Tensor\n",
    "    # cross_attn_scores: Optional[Tensor] = None：表示交叉注意力机制计算的注意力分数（cross-attention scores）。这个字段是可选的，只有在有交叉注意力时才会被使用。\n",
    "    cross_attn_scores: Optional[Tensor] = None\n",
    "\n",
    "# TransformerBlock 类，继承自 nn.Module，定义了一个标准的 Transformer 编码器块。\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config, add_cross_attention=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 从配置中获取超参数\n",
    "        self.hidden_size = config[\"d_model\"]  # 模型的隐藏层大小\n",
    "        self.num_heads = config[\"num_heads\"]  # 多头注意力机制的头数\n",
    "        dropout_rate = config[\"dropout\"]  # dropout 的比例\n",
    "        ffn_dim = config[\"dim_feedforward\"]  # 前馈神经网络（FFN）的维度\n",
    "        eps = config[\"layer_norm_eps\"]  # 层归一化的 epsilon 值\n",
    "\n",
    "        # 自注意力机制\n",
    "        self.self_atten = MultiHeadAttention(config)  # 多头自注意力\n",
    "        self.self_ln = nn.LayerNorm(self.hidden_size, eps=eps)  # 自注意力后的层归一化\n",
    "        self.self_dropout = nn.Dropout(dropout_rate)  # 自注意力后的 dropout\n",
    "\n",
    "        # 交叉注意力机制，仅在解码器中使用\n",
    "        if add_cross_attention:\n",
    "            self.cross_atten = MultiHeadAttention(config)  # 多头交叉注意力\n",
    "            self.cross_ln = nn.LayerNorm(self.hidden_size, eps=eps)  # 交叉注意力后的层归一化\n",
    "            self.cross_dropout = nn.Dropout(dropout_rate)  # 交叉注意力后的 dropout\n",
    "        else:\n",
    "            self.cross_atten = None  # 如果没有交叉注意力，则为 None\n",
    "\n",
    "        # 前馈神经网络（FFN）\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, ffn_dim),  # 输入经过一个线性层\n",
    "            nn.ReLU(),  # ReLU 激活函数\n",
    "            nn.Linear(ffn_dim, self.hidden_size),  # 输出通过另一个线性层\n",
    "        )\n",
    "        self.ffn_ln = nn.LayerNorm(self.hidden_size, eps=eps)  # FFN 输出后的层归一化\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)  # FFN 输出后的 dropout\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attn_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        cross_attn_mask=None,\n",
    "    ):\n",
    "        # 自注意力机制\n",
    "        self_atten_output = self.self_atten(\n",
    "            hidden_states, hidden_states, hidden_states, attn_mask\n",
    "        )\n",
    "        # self_atten_output 包含了 self_attn_scores 和 hidden_states\n",
    "        self_embeds = self.self_ln(\n",
    "            hidden_states + self.self_dropout(self_atten_output.hidden_states)\n",
    "        )  # 多头注意力输出后进行残差连接和层归一化\n",
    "\n",
    "        # 交叉注意力机制（仅在解码器中）\n",
    "        if self.cross_atten is not None:\n",
    "            assert encoder_outputs is not None  # 如果使用交叉注意力，必须传入 encoder 的输出\n",
    "            cross_atten_output = self.cross_atten(\n",
    "                self_embeds, encoder_outputs, encoder_outputs, cross_attn_mask\n",
    "            )  # query 是 self_embeds，key 和 value 是 encoder_outputs\n",
    "            cross_embeds = self.cross_ln(\n",
    "                self_embeds + self.cross_dropout(cross_atten_output.hidden_states)\n",
    "            )  # 交叉注意力输出后进行残差连接和层归一化\n",
    "\n",
    "        # 前馈神经网络（FFN）\n",
    "        embeds = cross_embeds if self.cross_atten is not None else self_embeds  # 如果有交叉注意力，则使用交叉注意力的输出；否则，使用自注意力输出\n",
    "        ffn_output = self.ffn(embeds)  # 前馈神经网络\n",
    "        embeds = self.ffn_ln(embeds + self.ffn_dropout(ffn_output))  # FFN 输出后进行残差连接和层归一化\n",
    "\n",
    "        # 返回包含 hidden_states（最终输出）、self_attn_scores 和 cross_attn_scores 的结果\n",
    "        return TransformerBlockOutput(\n",
    "            hidden_states=embeds,\n",
    "            self_attn_scores=self_atten_output.attn_scores,  # 返回自注意力的注意力分数\n",
    "            cross_attn_scores=cross_atten_output.attn_scores if self.cross_atten is not None else None,  # 返回交叉注意力的注意力分数（如果有交叉注意力）\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfJIGaohN4yE"
   },
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:13.402100Z",
     "iopub.status.busy": "2025-02-16T14:52:13.401686Z",
     "iopub.status.idle": "2025-02-16T14:52:13.409852Z",
     "shell.execute_reply": "2025-02-16T14:52:13.409154Z",
     "shell.execute_reply.started": "2025-02-16T14:52:13.402073Z"
    },
    "id": "sTLabHm7N4yE"
   },
   "source": [
    "from typing import List\n",
    "\n",
    "@dataclass # 定义一个dataclass类来存储TransformerEncoder的输出结果\n",
    "class TransformerEncoderOutput:\n",
    "    # 用于存储TransformerEncoder的输出结果，包含了：\n",
    "    # 1. 最后一层的隐藏状态\n",
    "    # 2. 每层的注意力分数，用于分析模型关注不同位置的程度。\n",
    "    last_hidden_states: Tensor  # 最后一层的隐藏状态输出，包含了经过所有编码层后得到的上下文信息\n",
    "    attn_scores: List[Tensor]   # 每层的自注意力分数，用于可视化模型在每一层的注意力分布\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        # 初始化Transformer编码器\n",
    "        super().__init__()\n",
    "        \n",
    "        # 从config中获取编码器的层数\n",
    "        self.num_layers = config[\"num_encoder_layers\"]  \n",
    "\n",
    "        # 使用nn.ModuleList来创建一个包含多个TransformerBlock的列表\n",
    "        # 每个TransformerBlock代表编码器中的一层\n",
    "        # ModuleList是一个可以存储多个子模块（TransformerBlock）的容器\n",
    "        # 在每一层中，输入会经过自注意力层和前馈神经网络层\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(self.num_layers)]  # 依据配置的层数创建每层TransformerBlock\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, encoder_inputs_embeds, attn_mask=None\n",
    "    ) -> TransformerEncoderOutput:\n",
    "        # 定义前向传播函数，encoder_inputs_embeds是输入的嵌入向量，通常是经过词嵌入和位置编码后的表示\n",
    "        attn_scores = []  # 用于存储每一层计算得到的自注意力分数\n",
    "        embeds = encoder_inputs_embeds  # 将输入的嵌入向量作为编码器的第一层输入\n",
    "\n",
    "        # 遍历每一层的TransformerBlock\n",
    "        for layer in self.layers:\n",
    "            block_outputs = layer(embeds, attn_mask=attn_mask)  # 将输入传入当前层的TransformerBlock\n",
    "            \n",
    "            # 更新embeds为当前层输出的hidden_states\n",
    "            # 作为下一层TransformerBlock的输入\n",
    "            embeds = block_outputs.hidden_states \n",
    "\n",
    "            # 每层的注意力分数存储到attn_scores中\n",
    "            # 这对于后续的可视化或调试非常有用\n",
    "            attn_scores.append(block_outputs.self_attn_scores)  # 每层的自注意力分数\n",
    "\n",
    "        # 返回最后一层的隐藏状态和所有层的注意力分数\n",
    "        return TransformerEncoderOutput(\n",
    "            last_hidden_states=embeds,  # 最后一层的输出作为最终的编码结果\n",
    "            attn_scores=attn_scores  # 每层的自注意力分数\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEMNj6eBN4yE"
   },
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:13.411620Z",
     "iopub.status.busy": "2025-02-16T14:52:13.411180Z",
     "iopub.status.idle": "2025-02-16T14:52:13.420117Z",
     "shell.execute_reply": "2025-02-16T14:52:13.419450Z",
     "shell.execute_reply.started": "2025-02-16T14:52:13.411584Z"
    },
    "id": "wCHOur-QN4yE",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.801353Z",
     "start_time": "2025-02-17T03:03:37.800354Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class TransformerDecoderOutput:\n",
    "    # 用于存储TransformerDecoder的输出结果，包含：\n",
    "    # 1. 最后一层的隐藏状态\n",
    "    # 2. 每层的自注意力分数\n",
    "    # 3. 每层的交叉注意力分数\n",
    "    last_hidden_states: Tensor  # 最后一层的隐藏状态，包含了经过所有解码器层后得到的上下文信息\n",
    "    self_attn_scores: List[Tensor]  # 每层的自注意力分数，用于分析解码器内部如何处理输入序列\n",
    "    cross_attn_scores: List[Tensor]  # 每层的交叉注意力分数，用于分析解码器如何与编码器的输出进行交互\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        # 初始化Transformer解码器\n",
    "        super().__init__()\n",
    "        \n",
    "        # 从config中获取解码器的层数\n",
    "        self.num_layers = config[\"num_decoder_layers\"]  \n",
    "\n",
    "        # 使用nn.ModuleList来创建一个包含多个TransformerBlock的列表\n",
    "        # 每个TransformerBlock代表解码器中的一层\n",
    "        # 在每一层中，输入会经过自注意力层和交叉注意力层，再经过前馈神经网络层\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(config, add_cross_attention=True)  # 解码器每层都有交叉注意力机制\n",
    "                for _ in range(self.num_layers)  # 根据配置的层数创建每层TransformerBlock\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, decoder_inputs_embeds, encoder_outputs, attn_mask=None, cross_attn_mask=None,) -> TransformerDecoderOutput:\n",
    "        # 定义前向传播函数，接受解码器的输入嵌入向量、编码器输出以及相应的注意力掩码\n",
    "        self_attn_scores = []  # 用于存储每一层计算得到的自注意力分数\n",
    "        cross_attn_scores = []  # 用于存储每一层计算得到的交叉注意力分数\n",
    "        embeds = decoder_inputs_embeds  # 将解码器输入的嵌入向量作为第一层输入\n",
    "\n",
    "        # 遍历每一层的TransformerBlock\n",
    "        for layer in self.layers:\n",
    "            # 通过当前的TransformerBlock进行前向传播\n",
    "            block_outputs = layer(\n",
    "                embeds,\n",
    "                attn_mask=attn_mask,  # 自注意力的掩码，用于在注意力计算时屏蔽一些无关的位置\n",
    "                encoder_outputs=encoder_outputs,  # 编码器的输出，用于交叉注意力\n",
    "                cross_attn_mask=cross_attn_mask,  # 交叉注意力的掩码，用于屏蔽编码器的无关位置\n",
    "            )\n",
    "            # 更新embeds为当前层输出的hidden_states，作为下一层TransformerBlock的输入\n",
    "            embeds = block_outputs.hidden_states \n",
    "\n",
    "            # 每层的自注意力分数存储到self_attn_scores列表中, 用于画图\n",
    "            self_attn_scores.append(block_outputs.self_attn_scores)\n",
    "            # 每层的交叉注意力分数存储到cross_attn_scores列表中, 用于画图\n",
    "            cross_attn_scores.append(block_outputs.cross_attn_scores)\n",
    "\n",
    "        # 返回最后一层的隐藏状态和每层的自注意力、交叉注意力分数\n",
    "        return TransformerDecoderOutput(\n",
    "            last_hidden_states=embeds,  # 最后一层的输出作为最终的解码器输出\n",
    "            self_attn_scores=self_attn_scores,  # 每层的自注意力分数\n",
    "            cross_attn_scores=cross_attn_scores,  # 每层的交叉注意力分数\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPrvXAXtN4yF"
   },
   "source": [
    "#### mask\n",
    "\n",
    "- mask实际上大类上只有两种\n",
    "    1. `padding_mask`：mask掉`pad_idx`，不计算损失\n",
    "    2. `attention_mask`：mask掉`pad_idx`，不计算注意力分数\n",
    "- Decoder的`attention_mask`和Encoder有一定的区别：\n",
    "    - Encoder可以同时看见序列所有信息，故只mask掉`pad_idx`\n",
    "    - Decoder只能看到在自身之前的序列的信息，故要额外mask掉自身之后的序列"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:13.421276Z",
     "iopub.status.busy": "2025-02-16T14:52:13.420974Z",
     "iopub.status.idle": "2025-02-16T14:52:13.427716Z",
     "shell.execute_reply": "2025-02-16T14:52:13.426896Z",
     "shell.execute_reply.started": "2025-02-16T14:52:13.421250Z"
    },
    "id": "D2N9VmcAWLn1",
    "outputId": "560b8af9-c854-4fd7-df3e-86e9eb9045cc",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.802353Z",
     "start_time": "2025-02-17T03:03:37.801353Z"
    }
   },
   "source": [
    "# 例子： 生成下三角的pad_mask\n",
    "(torch.triu(torch.ones(5, 5)) == 0).transpose(-1,-2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:13.429232Z",
     "iopub.status.busy": "2025-02-16T14:52:13.428804Z",
     "iopub.status.idle": "2025-02-16T14:52:13.634368Z",
     "shell.execute_reply": "2025-02-16T14:52:13.633548Z",
     "shell.execute_reply.started": "2025-02-16T14:52:13.429192Z"
    },
    "id": "QxpSYOsaN4yF",
    "outputId": "1022d3c3-c72e-4798-b998-c71c6690b94e",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.805349Z",
     "start_time": "2025-02-17T03:03:37.804353Z"
    }
   },
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    生成一个方形的后续掩码（Subsequent Mask）。\n",
    "    被屏蔽（mask）的部分填充为 True，未屏蔽的部分填充为 False。\n",
    "    \"\"\"\n",
    "    # torch.ones(sz, sz): 创建一个 sz × sz 的全 1 矩阵\n",
    "    # torch.triu(...): 取得上三角部分（主对角线以上部分为 1，以下部分为 0）\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 0).transpose(-1, -2).bool()\n",
    "    \n",
    "    return mask\n",
    "\n",
    "plt.matshow(generate_square_subsequent_mask(16))  # 生成 16x16 的掩码并可视化\n",
    "plt.colorbar()  # 显示颜色条\n",
    "plt.xlabel(\"keys\")  # x 轴代表键（key）\n",
    "plt.ylabel(\"querys\")  # y 轴代表查询（query）\n",
    "plt.title(\"1 means mask while 0 means unmask\")  # 标题：1 表示掩码，0 表示未被掩码\n",
    "plt.show()  # 展示掩码矩阵"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:13.635989Z",
     "iopub.status.busy": "2025-02-16T14:52:13.635607Z",
     "iopub.status.idle": "2025-02-16T14:52:14.305667Z",
     "shell.execute_reply": "2025-02-16T14:52:14.304906Z",
     "shell.execute_reply.started": "2025-02-16T14:52:13.635960Z"
    },
    "id": "cPeMjXO1N4yF",
    "outputId": "e6922ce8-ba71-491f-d7a6-709a14a4b7d7",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.806351Z",
     "start_time": "2025-02-17T03:03:37.806351Z"
    }
   },
   "source": [
    "# 例子：通过下面代码查看 mask（掩码）的效果\n",
    "# 输入的句子列表\n",
    "inputs_words = [\n",
    "    \"The quick brown fox jumps over the lazy dog .\",\n",
    "    \"What does the fox say ?\"\n",
    "]\n",
    "\n",
    "# 使用 tokenizer 对输入的句子进行编码，同时返回 mask\n",
    "# encode() 方法会将输入的文本转换为对应的 token ID，并返回相应的掩码（mask）\n",
    "inputs_ids, input_mask = tokenizer.encode(\n",
    "    [w.split() for w in inputs_words], return_mask=True\n",
    ")\n",
    "\n",
    "# 遍历每个输入句子，查看它们的编码和掩码效果\n",
    "for i in range(len(inputs_words)):\n",
    "    # 解码当前句子的 token ID，确保不会移除特殊 token（如BOS、EOS、PAD）\n",
    "    decode_text = tokenizer.decode(\n",
    "        inputs_ids[i: i+1].tolist(),\n",
    "        remove_bos=False,  # 不移除句首（BOS）\n",
    "        remove_eos=False,  # 不移除句尾（EOS）\n",
    "        remove_pad=False,  # 不移除填充（PAD）\n",
    "        split=True  # 以列表形式返回\n",
    "    )[0]\n",
    "\n",
    "    print(decode_text)  # 打印解码后的文本\n",
    "\n",
    "    # 生成自注意力掩码（self_attn_mask）\n",
    "    # input_mask[i] 形状为 (seq_len,)，先调整形状为 (1, seq_len)\n",
    "    # 然后重复扩展 seq_len 次，使其变成 (seq_len, seq_len) 形状\n",
    "    self_attn_mask = input_mask[i].reshape(1, -1).repeat_interleave(\n",
    "        inputs_ids.shape[-1], dim=0\n",
    "    )\n",
    "\n",
    "    # 生成未来信息屏蔽掩码（look_ahead_mask）\n",
    "    # 这是 Transformer 解码器中用于防止看到未来单词的掩码\n",
    "    look_ahead_mask = generate_square_subsequent_mask(inputs_ids.shape[-1])\n",
    "\n",
    "    # 绘制掩码矩阵的可视化\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # 创建 1 行 2 列的子图\n",
    "\n",
    "    # 绘制自注意力掩码（self-attention mask）\n",
    "    axs[0].matshow(self_attn_mask)  # 绘制掩码矩阵\n",
    "    axs[0].set_title(\"self_attn_mask\")  # 设置标题\n",
    "    axs[0].set_yticks(range(len(decode_text)), decode_text, fontsize=6)  # 设置 y 轴刻度\n",
    "    axs[0].set_ylabel(\"queries\")  # y 轴标签（查询）\n",
    "    axs[0].set_xticks(range(len(decode_text)), decode_text, fontsize=6)  # 设置 x 轴刻度\n",
    "    axs[0].set_xlabel(\"keys\")  # x 轴标签（键）\n",
    "\n",
    "    # 绘制未来信息屏蔽掩码（look-ahead mask）\n",
    "    axs[1].matshow(look_ahead_mask)  # 绘制掩码矩阵\n",
    "    axs[1].set_title(\"look_ahead_mask\")  # 设置标题\n",
    "    axs[1].set_yticks(range(len(decode_text)), decode_text, fontsize=6)  # 设置 y 轴刻度\n",
    "    axs[1].set_ylabel(\"queries\")  # y 轴标签（查询）\n",
    "    axs[1].set_xticks(range(len(decode_text)), decode_text, fontsize=6)  # 设置 x 轴刻度\n",
    "    axs[1].set_xlabel(\"keys\")  # x 轴标签（键）\n",
    "\n",
    "    plt.show()  # 显示图像\n",
    "\n",
    "    print('-' * 50)  # 分隔符，方便阅读输出结果"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:14.306917Z",
     "iopub.status.busy": "2025-02-16T14:52:14.306569Z",
     "iopub.status.idle": "2025-02-16T14:52:14.312675Z",
     "shell.execute_reply": "2025-02-16T14:52:14.311851Z",
     "shell.execute_reply.started": "2025-02-16T14:52:14.306888Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.808352Z",
     "start_time": "2025-02-17T03:03:37.807349Z"
    }
   },
   "source": [
    "# 例子：我随机两个[5, 1, 1, 4]与[1, 1, 4, 4]尺寸的张量，并求和\n",
    "a = torch.randn(5, 1, 1, 4)\n",
    "b = torch.randn(1, 1, 4, 4)\n",
    "(a + b).shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TelCDOoEN4yF"
   },
   "source": [
    "#### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:14.314102Z",
     "iopub.status.busy": "2025-02-16T14:52:14.313577Z",
     "iopub.status.idle": "2025-02-16T14:52:14.334051Z",
     "shell.execute_reply": "2025-02-16T14:52:14.333239Z",
     "shell.execute_reply.started": "2025-02-16T14:52:14.314075Z"
    },
    "id": "oFGNt8FPN4yF"
   },
   "source": [
    "@dataclass \n",
    "class TransformerOutput:\n",
    "    logits: Tensor  # 预测的输出 logits，表示模型对每个词的概率分布\n",
    "    encoder_last_hidden_states: Tensor  # 编码器的最终隐藏状态\n",
    "    encoder_attn_scores: List[Tensor]  # 编码器的自注意力得分（用于可视化）\n",
    "    decoder_last_hidden_states: Tensor  # 解码器的最终隐藏状态\n",
    "    decoder_self_attn_scores: List[Tensor]  # 解码器的自注意力得分（用于可视化）\n",
    "    decoder_cross_attn_scores: List[Tensor]  # 解码器的交叉注意力得分（用于可视化）\n",
    "    preds: Optional[Tensor] = None  # 推理时的最终预测结果\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 超参数（Transformer的关键参数）\n",
    "        self.hidden_size = config[\"d_model\"]  # 模型的隐藏层大小\n",
    "        self.num_encoder_layers = config[\"num_encoder_layers\"]  # 编码器层数\n",
    "        self.num_decoder_layers = config[\"num_decoder_layers\"]  # 解码器层数\n",
    "        self.pad_idx = config[\"pad_idx\"]  # PAD标记索引\n",
    "        self.bos_idx = config[\"bos_idx\"]  # 句子起始标记索引\n",
    "        self.eos_idx = config[\"eos_idx\"]  # 句子结束标记索引\n",
    "        self.vocab_size = config[\"vocab_size\"]  # 词汇表大小\n",
    "        self.dropout_rate = config[\"dropout\"]  # dropout 比例\n",
    "        self.max_length = config[\"max_length\"]  # 生成的最大句子长度\n",
    "        self.share = config[\"share_embedding\"]  # 是否共享词嵌入\n",
    "\n",
    "        # 词嵌入层\n",
    "        self.src_embedding = TransformerEmbedding(config)  # 源语言嵌入层\n",
    "        if self.share:  # 如果共享词嵌入，则目标语言的嵌入层与源语言相同\n",
    "            self.trg_embedding = self.src_embedding  # 共享参数，节省内存\n",
    "            self.linear = lambda x: torch.matmul(\n",
    "                x, self.trg_embedding.get_word_embedding_weights().T\n",
    "            )  # 输出层共享参数，使用嵌入矩阵的转置\n",
    "        else:\n",
    "            self.trg_embedding = TransformerEmbedding(config)  # 目标语言的嵌入层\n",
    "            self.linear = nn.Linear(self.hidden_size, self.vocab_size)  # 独立的输出层\n",
    "\n",
    "        # Transformer 编码器和解码器\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.decoder = TransformerDecoder(config)\n",
    "\n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"使用 Xavier 均匀分布初始化权重\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        生成一个下三角掩码矩阵（用于自回归解码）\n",
    "        被掩码的位置填充 True，未掩码的位置填充 False\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 0).transpose(-1, -2).bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(\n",
    "        self, encoder_inputs, decoder_inputs, encoder_inputs_mask=None\n",
    "    ) -> TransformerOutput:\n",
    "        \"\"\"\n",
    "        Transformer 模型的前向传播\n",
    "        encoder_inputs: [batch_size, src_len] 输入的源语言序列\n",
    "        decoder_inputs: [batch_size, trg_len] 输入的目标语言序列\n",
    "        encoder_inputs_mask: [batch_size, src_len] 源语言的掩码\n",
    "        \"\"\"\n",
    "        if encoder_inputs_mask is None:\n",
    "            encoder_inputs_mask = encoder_inputs.eq(self.pad_idx)  # [batch_size, src_len] pad 位置为 True\n",
    "        encoder_inputs_mask = encoder_inputs_mask.unsqueeze(1).unsqueeze(2)  # 适配多头注意力的输入形状 [batch_size, 1, 1, src_len]\n",
    "\n",
    "        # 生成解码器的掩码\n",
    "        look_ahead_mask = self.generate_square_subsequent_mask(decoder_inputs.shape[1])\n",
    "        look_ahead_mask = (\n",
    "            look_ahead_mask.unsqueeze(0).unsqueeze(0).to(decoder_inputs.device)\n",
    "        )  # 形状变换 [1, 1, trg_len, trg_len]\n",
    "        decoder_inputs_mask = decoder_inputs.eq(self.pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, trg_len]\n",
    "        decoder_inputs_mask = decoder_inputs_mask + look_ahead_mask  # 组合两种掩码\n",
    "\n",
    "        # 编码\n",
    "        encoder_inputs_embeds = self.src_embedding(encoder_inputs)\n",
    "        encoder_outputs = self.encoder(encoder_inputs_embeds, encoder_inputs_mask)\n",
    "\n",
    "        # 解码\n",
    "        decoder_inputs_embeds = self.trg_embedding(decoder_inputs)\n",
    "        decoder_outputs = self.decoder(\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            encoder_outputs=encoder_outputs.last_hidden_states,\n",
    "            attn_mask=decoder_inputs_mask,\n",
    "            cross_attn_mask=encoder_inputs_mask,\n",
    "        )\n",
    "\n",
    "        logits = self.linear(decoder_outputs.last_hidden_states)  # 计算最终输出的 logits\n",
    "\n",
    "        return TransformerOutput(\n",
    "            logits=logits,\n",
    "            encoder_last_hidden_states=encoder_outputs.last_hidden_states,\n",
    "            encoder_attn_scores=encoder_outputs.attn_scores,\n",
    "            decoder_last_hidden_states=decoder_outputs.last_hidden_states,\n",
    "            decoder_self_attn_scores=decoder_outputs.self_attn_scores,\n",
    "            decoder_cross_attn_scores=decoder_outputs.cross_attn_scores,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer(self, encoder_inputs, encoder_inputs_mask=None) -> Tensor:\n",
    "        \"\"\" Transformer 推理 \"\"\"\n",
    "        if encoder_inputs_mask is None:\n",
    "            encoder_inputs_mask = encoder_inputs.eq(self.pad_idx)\n",
    "        encoder_inputs_mask = encoder_inputs_mask.unsqueeze(1).unsqueeze(2)\n",
    "        look_ahead_mask = self.generate_square_subsequent_mask(self.max_length)\n",
    "        look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(0).to(encoder_inputs.device)\n",
    "\n",
    "        # 编码过程\n",
    "        encoder_inputs_embeds = self.src_embedding(encoder_inputs)\n",
    "        encoder_outputs = self.encoder(encoder_inputs_embeds)\n",
    "\n",
    "        # 解码过程（自回归）\n",
    "        decoder_inputs = torch.Tensor([self.bos_idx] * encoder_inputs.shape[0]).reshape(-1, 1).long().to(device=encoder_inputs.device)\n",
    "        for cur_len in tqdm(range(1, self.max_length + 1)):\n",
    "            decoder_inputs_embeds = self.trg_embedding(decoder_inputs)\n",
    "            decoder_outputs = self.decoder(\n",
    "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                encoder_outputs=encoder_outputs.last_hidden_states,\n",
    "                attn_mask=look_ahead_mask[:, :, :cur_len, :cur_len],\n",
    "            )\n",
    "            logits = self.linear(decoder_outputs.last_hidden_states)\n",
    "            next_token = logits.argmax(dim=-1)[:, -1:]\n",
    "            decoder_inputs = torch.cat([decoder_inputs, next_token], dim=-1)\n",
    "            if all((decoder_inputs == self.eos_idx).sum(dim=-1) > 0):\n",
    "                break\n",
    "\n",
    "        return TransformerOutput(\n",
    "            preds=decoder_inputs[:, 1:],\n",
    "            logits=logits,\n",
    "            encoder_last_hidden_states=encoder_outputs.last_hidden_states,\n",
    "            encoder_attn_scores=encoder_outputs.attn_scores,\n",
    "            decoder_last_hidden_states=decoder_outputs.last_hidden_states,\n",
    "            decoder_self_attn_scores=decoder_outputs.self_attn_scores,\n",
    "            decoder_cross_attn_scores=decoder_outputs.cross_attn_scores,\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXHQ3sGTN4yG"
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxNA8DIzN4yG"
   },
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:14.335341Z",
     "iopub.status.busy": "2025-02-16T14:52:14.335073Z",
     "iopub.status.idle": "2025-02-16T14:52:14.341649Z",
     "shell.execute_reply": "2025-02-16T14:52:14.340814Z",
     "shell.execute_reply.started": "2025-02-16T14:52:14.335316Z"
    },
    "id": "1xzahHANN4yG"
   },
   "source": [
    "class CrossEntropyWithPadding:\n",
    "    def __init__(self, config):\n",
    "        # 初始化时读取配置中的标签平滑参数\n",
    "        self.label_smoothing = config[\"label_smoothing\"]\n",
    "\n",
    "    def __call__(self, logits, labels, padding_mask=None):\n",
    "        # logits.shape = [batch size, sequence length, num of classes]\n",
    "        # labels.shape = [batch size, sequence length]\n",
    "        # padding_mask.shape = [batch size, sequence length]\n",
    "\n",
    "        bs, seq_len, nc = logits.shape  # 获取批次大小、序列长度和类别数\n",
    "        # 计算交叉熵损失\n",
    "        # 将logits调整为[batch size * sequence length, num of classes]的形状\n",
    "        # 将labels调整为一维，大小为[batch size * sequence length]\n",
    "        # 如果有标签平滑，交叉熵损失会被平滑\n",
    "        loss = F.cross_entropy(logits.reshape(bs * seq_len, nc), labels.reshape(-1), reduce=False, label_smoothing=self.label_smoothing)\n",
    "        # 如果没有padding_mask，则直接返回平均损失\n",
    "        if padding_mask is None:\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            # 如果有padding_mask，将其调整为一维张量，mask部分为0，非mask部分为1\n",
    "            padding_mask = 1 - padding_mask.reshape(-1)\n",
    "            # 对于非padding部分的损失进行加权求和\n",
    "            # padding_mask为0的部分损失会被忽略，padding_mask为1的部分损失会参与计算\n",
    "            loss = torch.mul(loss, padding_mask).sum() / padding_mask.sum()\n",
    "\n",
    "        return loss  # 返回最终的损失值"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuBC9KqFN4yG"
   },
   "source": [
    "### 学习率衰减"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:14.342864Z",
     "iopub.status.busy": "2025-02-16T14:52:14.342546Z",
     "iopub.status.idle": "2025-02-16T14:52:14.501745Z",
     "shell.execute_reply": "2025-02-16T14:52:14.501044Z",
     "shell.execute_reply.started": "2025-02-16T14:52:14.342830Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "x=np.arange(1, 40000)\n",
    "plt.plot(x, x * (4000 ** (-1.5)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:14.503025Z",
     "iopub.status.busy": "2025-02-16T14:52:14.502683Z",
     "iopub.status.idle": "2025-02-16T14:52:14.508129Z",
     "shell.execute_reply": "2025-02-16T14:52:14.507357Z",
     "shell.execute_reply.started": "2025-02-16T14:52:14.502996Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.814352Z",
     "start_time": "2025-02-17T03:03:37.813351Z"
    }
   },
   "source": [
    "np.sqrt(512)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:14.509262Z",
     "iopub.status.busy": "2025-02-16T14:52:14.508921Z",
     "iopub.status.idle": "2025-02-16T14:52:14.647170Z",
     "shell.execute_reply": "2025-02-16T14:52:14.646456Z",
     "shell.execute_reply.started": "2025-02-16T14:52:14.509235Z"
    },
    "id": "UQPiKK4nN4yG",
    "outputId": "63cfb132-ef75-4f94-f0c8-86a2f36e09c2",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.815352Z",
     "start_time": "2025-02-17T03:03:37.815352Z"
    }
   },
   "source": [
    "# NoamDecayScheduler 是一个自定义或外部定义的学习率衰减调度器类。它需要接收配置 config 作为参数，可能实现了特定的学习率衰减方案\n",
    "class NoamDecayScheduler:\n",
    "    def __init__(self, config):\n",
    "        self.d_model = config[\"d_model\"]\n",
    "        self.warmup_steps = config[\"warmup_steps\"]\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step += 1\n",
    "        arg1 = step ** (-0.5) #4000步之后是arg1\n",
    "        arg2 = step * (self.warmup_steps ** (-1.5))  #4000步之前是arg2\n",
    "\n",
    "        arg3 = self.d_model ** (-0.5)\n",
    "\n",
    "        return arg3 * np.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "temp_learning_rate_schedule = NoamDecayScheduler({\"d_model\": 512, \"warmup_steps\": 4000})\n",
    "#下面是学习率的设计图\n",
    "plt.plot(temp_learning_rate_schedule(np.arange(0, 40000)))\n",
    "plt.ylabel(\"Leraning rate\")\n",
    "plt.xlabel(\"Train step\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHj_EjzlN4yW"
   },
   "source": [
    "### 优化器"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:14.648401Z",
     "iopub.status.busy": "2025-02-16T14:52:14.648062Z",
     "iopub.status.idle": "2025-02-16T14:52:14.653579Z",
     "shell.execute_reply": "2025-02-16T14:52:14.652924Z",
     "shell.execute_reply.started": "2025-02-16T14:52:14.648373Z"
    },
    "id": "1EVLKx2rN4yW"
   },
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import Adam\n",
    "\n",
    "def get_optimizer(model, config):\n",
    "    base_lr = 0.1\n",
    "    beta1 = config[\"beta1\"] # Adam 的 beta1\n",
    "    beta2 = config[\"beta2\"] # Adam 的 beta2\n",
    "    eps = config[\"eps\"]\n",
    "    optimizer = Adam(model.parameters(), lr=base_lr, betas=(beta1, beta2), eps=eps)\n",
    "    lr_scheduler = NoamDecayScheduler(config) #config是一个字典，包含了学习率衰减的参数\n",
    "    # 使用 LambdaLR 调度器，它可以根据给定的函数 lr_lambda 调整学习率。这里将 lr_scheduler 作为函数传递给 LambdaLR，它包含了特定于模型或任务的学习率调度规则\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lr_scheduler)\n",
    "    return optimizer, scheduler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBhiO7O2N4yW"
   },
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:14.654899Z",
     "iopub.status.busy": "2025-02-16T14:52:14.654573Z",
     "iopub.status.idle": "2025-02-16T14:52:17.061512Z",
     "shell.execute_reply": "2025-02-16T14:52:17.060654Z",
     "shell.execute_reply.started": "2025-02-16T14:52:14.654871Z"
    },
    "id": "rWFMJwBkN4yX",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.818351Z",
     "start_time": "2025-02-17T03:03:37.818351Z"
    }
   },
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TensorBoardCallback:\n",
    "    def __init__(self, log_dir, flush_secs=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            log_dir (str): dir to write log.\n",
    "            flush_secs (int, optional): write to dsk each flush_secs seconds. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.writer = SummaryWriter(log_dir=log_dir, flush_secs=flush_secs)\n",
    "\n",
    "    def draw_model(self, model, input_shape):\n",
    "        self.writer.add_graph(model, input_to_model=torch.randn(input_shape))\n",
    "\n",
    "    def add_loss_scalars(self, step, loss, val_loss):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/loss\",\n",
    "            tag_scalar_dict={\"loss\": loss, \"val_loss\": val_loss},\n",
    "            global_step=step,\n",
    "            )\n",
    "\n",
    "    def add_acc_scalars(self, step, acc, val_acc):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/accuracy\",\n",
    "            tag_scalar_dict={\"accuracy\": acc, \"val_accuracy\": val_acc},\n",
    "            global_step=step,\n",
    "        )\n",
    "\n",
    "    def add_lr_scalars(self, step, learning_rate):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/learning_rate\",\n",
    "            tag_scalar_dict={\"learning_rate\": learning_rate},\n",
    "            global_step=step,\n",
    "\n",
    "        )\n",
    "\n",
    "    def __call__(self, step, **kwargs):\n",
    "        # add loss\n",
    "        loss = kwargs.pop(\"loss\", None)\n",
    "        val_loss = kwargs.pop(\"val_loss\", None)\n",
    "        if loss is not None and val_loss is not None:\n",
    "            self.add_loss_scalars(step, loss, val_loss)\n",
    "        # add acc\n",
    "        acc = kwargs.pop(\"acc\", None)\n",
    "        val_acc = kwargs.pop(\"val_acc\", None)\n",
    "        if acc is not None and val_acc is not None:\n",
    "            self.add_acc_scalars(step, acc, val_acc)\n",
    "        # add lr\n",
    "        learning_rate = kwargs.pop(\"lr\", None)\n",
    "        if learning_rate is not None:\n",
    "            self.add_lr_scalars(step, learning_rate)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:17.063065Z",
     "iopub.status.busy": "2025-02-16T14:52:17.062489Z",
     "iopub.status.idle": "2025-02-16T14:52:17.069753Z",
     "shell.execute_reply": "2025-02-16T14:52:17.068747Z",
     "shell.execute_reply.started": "2025-02-16T14:52:17.063034Z"
    },
    "id": "64y_NBHMN4yX",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.820351Z",
     "start_time": "2025-02-17T03:03:37.819353Z"
    }
   },
   "source": [
    "class SaveCheckpointsCallback:\n",
    "    def __init__(self, save_dir, save_step=5000, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Save checkpoints each save_epoch epoch.\n",
    "        We save checkpoint by epoch in this implementation.\n",
    "        Usually, training scripts with pytorch evaluating model and save checkpoint by step.\n",
    "\n",
    "        Args:\n",
    "            save_dir (str): dir to save checkpoint\n",
    "            save_epoch (int, optional): the frequency to save checkpoint. Defaults to 1.\n",
    "            save_best_only (bool, optional): If True, only save the best model or save each model at every epoch.\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        self.save_step = save_step\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_metrics = - np.inf\n",
    "\n",
    "        # mkdir\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "\n",
    "    def __call__(self, step, state_dict, metric=None):\n",
    "        if step % self.save_step > 0:\n",
    "            return\n",
    "\n",
    "        if self.save_best_only:\n",
    "            assert metric is not None\n",
    "            if metric >= self.best_metrics:\n",
    "                # save checkpoints\n",
    "                torch.save(state_dict, os.path.join(self.save_dir, \"best.ckpt\"))\n",
    "                # update best metrics\n",
    "                self.best_metrics = metric\n",
    "        else:\n",
    "            torch.save(state_dict, os.path.join(self.save_dir, f\"{step}.ckpt\"))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:17.071117Z",
     "iopub.status.busy": "2025-02-16T14:52:17.070786Z",
     "iopub.status.idle": "2025-02-16T14:52:17.076313Z",
     "shell.execute_reply": "2025-02-16T14:52:17.075653Z",
     "shell.execute_reply.started": "2025-02-16T14:52:17.071090Z"
    },
    "id": "Uk4PEb70N4yX",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.822003Z",
     "start_time": "2025-02-17T03:03:37.821352Z"
    }
   },
   "source": [
    "class EarlyStopCallback:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            patience (int, optional): Number of epochs with no improvement after which training will be stopped.. Defaults to 5.\n",
    "            min_delta (float, optional): Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute\n",
    "                change of less than min_delta, will count as no improvement. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = - np.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric >= self.best_metric + self.min_delta:\n",
    "            # update best metric\n",
    "            self.best_metric = metric\n",
    "            # reset counter\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        return self.counter >= self.patience\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AB84Qx1N4yX"
   },
   "source": [
    "### training & valuating"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:17.077546Z",
     "iopub.status.busy": "2025-02-16T14:52:17.077116Z",
     "iopub.status.idle": "2025-02-16T14:52:17.082698Z",
     "shell.execute_reply": "2025-02-16T14:52:17.081915Z",
     "shell.execute_reply.started": "2025-02-16T14:52:17.077519Z"
    },
    "id": "1mKPSFkON4yX",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.823520Z",
     "start_time": "2025-02-17T03:03:37.822511Z"
    }
   },
   "source": [
    "@torch.no_grad()\n",
    "def evaluating(model, dataloader, loss_fct):\n",
    "    loss_list = []\n",
    "    for batch in dataloader:\n",
    "        encoder_inputs = batch[\"encoder_inputs\"]\n",
    "        encoder_inputs_mask = batch[\"encoder_inputs_mask\"]\n",
    "        decoder_inputs = batch[\"decoder_inputs\"]\n",
    "        decoder_labels = batch[\"decoder_labels\"]\n",
    "        decoder_labels_mask = batch[\"decoder_labels_mask\"]\n",
    "\n",
    "        # 前向计算\n",
    "        outputs = model(\n",
    "            encoder_inputs=encoder_inputs,\n",
    "            decoder_inputs=decoder_inputs,\n",
    "            encoder_inputs_mask=encoder_inputs_mask\n",
    "            )\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fct(logits, decoder_labels, padding_mask=decoder_labels_mask)         # 验证集损失\n",
    "        loss_list.append(loss.cpu().item())\n",
    "\n",
    "    return np.mean(loss_list)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:17.083953Z",
     "iopub.status.busy": "2025-02-16T14:52:17.083585Z",
     "iopub.status.idle": "2025-02-16T14:52:17.094823Z",
     "shell.execute_reply": "2025-02-16T14:52:17.094130Z",
     "shell.execute_reply.started": "2025-02-16T14:52:17.083927Z"
    },
    "id": "PClBmtgWN4yY",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.825519Z",
     "start_time": "2025-02-17T03:03:37.824518Z"
    }
   },
   "source": [
    "# 训练\n",
    "def training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epoch,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    tensorboard_callback=None,\n",
    "    save_ckpt_callback=None,\n",
    "    early_stop_callback=None,\n",
    "    eval_step=500,\n",
    "    ):\n",
    "    record_dict = {\n",
    "        \"train\": [],\n",
    "        \"val\": []\n",
    "    }\n",
    "\n",
    "    global_step = 1\n",
    "    model.train()\n",
    "    with tqdm(total=epoch * len(train_loader)) as pbar:\n",
    "        for epoch_id in range(epoch):\n",
    "            # training\n",
    "            for batch in train_loader:\n",
    "                encoder_inputs = batch[\"encoder_inputs\"]\n",
    "                encoder_inputs_mask = batch[\"encoder_inputs_mask\"]\n",
    "                decoder_inputs = batch[\"decoder_inputs\"]\n",
    "                decoder_labels = batch[\"decoder_labels\"]\n",
    "                decoder_labels_mask = batch[\"decoder_labels_mask\"]\n",
    "                # 梯度清空\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 前向计算\n",
    "                outputs = model(\n",
    "                    encoder_inputs=encoder_inputs,\n",
    "                    decoder_inputs=decoder_inputs,\n",
    "                    encoder_inputs_mask=encoder_inputs_mask\n",
    "                    )\n",
    "                logits = outputs.logits\n",
    "                loss = loss_fct(logits, decoder_labels, padding_mask=decoder_labels_mask)\n",
    "\n",
    "                # 梯度回传\n",
    "                loss.backward()\n",
    "\n",
    "                # 调整优化器，包括学习率的变动等\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step() # 更新学习率\n",
    "\n",
    "                loss = loss.cpu().item()\n",
    "                # record\n",
    "                record_dict[\"train\"].append({\n",
    "                    \"loss\": loss, \"step\": global_step\n",
    "                })\n",
    "\n",
    "                # evaluating\n",
    "                if global_step % eval_step == 0:\n",
    "                    model.eval()\n",
    "                    val_loss = evaluating(model, val_loader, loss_fct)\n",
    "                    record_dict[\"val\"].append({\n",
    "                        \"loss\": val_loss, \"step\": global_step\n",
    "                    })\n",
    "                    model.train()\n",
    "\n",
    "                    # 1. 使用 tensorboard 可视化\n",
    "                    cur_lr = optimizer.param_groups[0][\"lr\"] if scheduler is None else scheduler.get_last_lr()[0]\n",
    "                    if tensorboard_callback is not None:\n",
    "                        tensorboard_callback(\n",
    "                            global_step,\n",
    "                            loss=loss, val_loss=val_loss,\n",
    "                            lr=cur_lr,\n",
    "                            )\n",
    "\n",
    "                    # 2. 保存模型权重 save model checkpoint\n",
    "                    if save_ckpt_callback is not None:\n",
    "                        save_ckpt_callback(global_step, model.state_dict(), metric=-val_loss)\n",
    "\n",
    "                    # 3. 早停 Early Stop\n",
    "                    if early_stop_callback is not None:\n",
    "                        early_stop_callback(-val_loss)\n",
    "                        if early_stop_callback.early_stop:\n",
    "                            print(f\"Early stop at epoch {epoch_id} / global_step {global_step}\")\n",
    "                            return record_dict\n",
    "\n",
    "                # udate step\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "            pbar.set_postfix({\"epoch\": epoch_id, \"loss\": loss, \"val_loss\": val_loss})\n",
    "\n",
    "    return record_dict\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:17.096165Z",
     "iopub.status.busy": "2025-02-16T14:52:17.095804Z",
     "iopub.status.idle": "2025-02-16T14:52:17.209654Z",
     "shell.execute_reply": "2025-02-16T14:52:17.208904Z",
     "shell.execute_reply.started": "2025-02-16T14:52:17.096139Z"
    },
    "id": "khZqMS8pN4yY",
    "outputId": "2367a363-585b-4321-f7ea-cb11d9623008",
    "ExecuteTime": {
     "end_time": "2025-02-17T03:03:37.827518Z",
     "start_time": "2025-02-17T03:03:37.826519Z"
    }
   },
   "source": [
    "# 配置模型的超参数\n",
    "config = {\n",
    "    \"bos_idx\": 1,  # 句子起始标记 (Beginning of Sentence)\n",
    "    \"eos_idx\": 3,  # 句子结束标记 (End of Sentence)\n",
    "    \"pad_idx\": 0,  # 填充标记 (Padding Index)\n",
    "    \"vocab_size\": len(word2idx),  # 词汇表大小\n",
    "    \"max_length\": 128,  # 句子最大长度\n",
    "    \"d_model\": 512,  # Transformer 词向量的维度\n",
    "    \"dim_feedforward\": 2048,  # 前馈神经网络（FFN）的隐藏层大小\n",
    "    \"dropout\": 0.1,  # dropout 率，用于防止过拟合\n",
    "    \"layer_norm_eps\": 1e-6,  # 层归一化 (Layer Normalization) 时的 epsilon，防止除零错误\n",
    "    \"num_heads\": 8,  # 多头注意力机制的头数\n",
    "    \"num_decoder_layers\": 6,  # 解码器层数\n",
    "    \"num_encoder_layers\": 6,  # 编码器层数\n",
    "    \"label_smoothing\": 0.1,  # 交叉熵损失中的标签平滑系数\n",
    "    \"beta1\": 0.9,  # Adam 优化器的 beta1 参数（用于一阶矩估计）\n",
    "    \"beta2\": 0.98,  # Adam 优化器的 beta2 参数（用于二阶矩估计）\n",
    "    \"eps\": 1e-9,  # Adam 优化器的 epsilon，防止除零错误\n",
    "    \"warmup_steps\": 4_000,  # 学习率预热步数\n",
    "    \"share_embedding\": False,  # 是否在编码器和解码器之间共享词向量\n",
    "}\n",
    "\n",
    "def get_dl(dataset, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    获取数据加载器 (DataLoader)\n",
    "\n",
    "    参数：\n",
    "    dataset: 训练或验证数据集\n",
    "    batch_size: 批次大小\n",
    "    shuffle: 是否对批次进行随机排序（默认 True）\n",
    "\n",
    "    返回：\n",
    "    DataLoader 对象\n",
    "    \"\"\"\n",
    "    # 使用 Transformer 任务自定义的批次采样器，确保 batch 内的句子长度一致\n",
    "    sampler = TransformerBatchSampler(dataset, batch_size=batch_size, shuffle_batch=shuffle)\n",
    "    # 使用 DataLoader 进行数据加载，collate_fn 用于处理不同长度的句子\n",
    "    sample_dl = DataLoader(dataset, batch_sampler=sampler, collate_fn=partial(collate_fct, tokenizer=tokenizer))\n",
    "    return sample_dl\n",
    "\n",
    "# 构建训练集和验证集\n",
    "train_ds = LangPairDataset(\"train\", max_length=config[\"max_length\"])  # 训练数据集\n",
    "val_ds = LangPairDataset(\"val\", max_length=config[\"max_length\"])  # 验证数据集\n",
    "\n",
    "# 构建分词器（Tokenizer），用于将文本转换为索引\n",
    "tokenizer = Tokenizer(word2idx=word2idx, idx2word=idx2word, max_length=config[\"max_length\"])\n",
    "\n",
    "# 训练的批次大小\n",
    "batch_size = 2048\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dl = get_dl(train_ds, batch_size=batch_size, shuffle=True)  # 训练数据加载器\n",
    "val_dl = get_dl(val_ds, batch_size=batch_size, shuffle=False)  # 验证数据加载器"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:17.210929Z",
     "iopub.status.busy": "2025-02-16T14:52:17.210517Z",
     "iopub.status.idle": "2025-02-16T14:52:18.466975Z",
     "shell.execute_reply": "2025-02-16T14:52:18.466177Z",
     "shell.execute_reply.started": "2025-02-16T14:52:17.210898Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#计算模型参数量\n",
    "model = TransformerModel(config)\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:18.468297Z",
     "iopub.status.busy": "2025-02-16T14:52:18.467869Z",
     "iopub.status.idle": "2025-02-16T14:52:18.473708Z",
     "shell.execute_reply": "2025-02-16T14:52:18.472873Z",
     "shell.execute_reply.started": "2025-02-16T14:52:18.468263Z"
    },
    "id": "63slV04gWLn8",
    "outputId": "5c5ede03-0f6f-48d5-a177-d961886ec340"
   },
   "source": [
    "config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "55146ac7395840dca847e0f63aa80561",
      "abb9bc76c0ce4634bd340be20c074b8c",
      "a89d790df59245fcaf26335d4903c9b4",
      "f837255914194dc792cf1514dee03dc5",
      "46a51a1719d8444eb5877720253d3c37",
      "013d2a6b90144432bbfb15136235faba",
      "2b82e42c34804b13bc83be4c7009d4e2",
      "9233439bdd6746c28ad4a20ef47799f6",
      "0e10428326bc436e8dbc079e89570bde",
      "0fe4ed7283c04496affe727c235d31c4",
      "0c0c7229347b49ffbb9a0f7bd41199b9"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T14:52:18.474923Z",
     "iopub.status.busy": "2025-02-16T14:52:18.474570Z",
     "iopub.status.idle": "2025-02-16T14:52:21.235438Z",
     "shell.execute_reply": "2025-02-16T14:52:21.234479Z",
     "shell.execute_reply.started": "2025-02-16T14:52:18.474896Z"
    },
    "id": "tVO2zJ04N4yY",
    "outputId": "7d4079f5-65d4-4da1-9001-91aa5525250e"
   },
   "source": [
    "epoch = 20\n",
    "\n",
    "# model\n",
    "model = TransformerModel(config)\n",
    "# 1. 定义损失函数 采用交叉熵损失\n",
    "loss_fct = CrossEntropyWithPadding(config)\n",
    "# 2. 定义优化器 采用 adam\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer, scheduler = get_optimizer(model, config)\n",
    "\n",
    "# 1. tensorboard 可视化\n",
    "if not os.path.exists(\"runs\"):\n",
    "    os.mkdir(\"runs\")\n",
    "exp_name = \"translate-transformer-{}\".format(\"share\" if config[\"share_embedding\"] else \"not-share\")\n",
    "tensorboard_callback = TensorBoardCallback(f\"runs/{exp_name}\")\n",
    "# tensorboard_callback.draw_model(model, [1, MAX_LENGTH])\n",
    "# 2. save best\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.makedirs(\"checkpoints\")\n",
    "save_ckpt_callback = SaveCheckpointsCallback(\n",
    "    f\"checkpoints/{exp_name}\", save_step=500, save_best_only=True)\n",
    "# 3. early stop\n",
    "early_stop_callback = EarlyStopCallback(patience=8)\n",
    "\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T14:19:42.277160Z",
     "start_time": "2025-02-16T14:19:23.333725Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T15:02:08.107654Z",
     "iopub.status.busy": "2025-02-16T15:02:08.107127Z",
     "iopub.status.idle": "2025-02-16T15:19:27.417039Z",
     "shell.execute_reply": "2025-02-16T15:19:27.416155Z",
     "shell.execute_reply.started": "2025-02-16T15:02:08.107611Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20840/20840 [17:19<00:00, 20.05it/s, epoch=19, loss=2.74, val_loss=3.44]\n"
     ]
    }
   ],
   "source": [
    "record = training(\n",
    "    model,\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    epoch,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tensorboard_callback=tensorboard_callback,\n",
    "    save_ckpt_callback=save_ckpt_callback,\n",
    "    early_stop_callback=early_stop_callback,\n",
    "    eval_step=500\n",
    "    )\n",
    "\n",
    "# Training took 3.5 days on 8 P100 GPUs\n",
    "# We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTjbeFlIN4yZ"
   },
   "source": [
    "## 推理\n",
    "\n",
    "- 翻译项目的评估指标一般是BLEU4，感兴趣的同学自行了解并实现\n",
    "- 接下来进行翻译推理，并作出注意力的热度图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:22:52.711846Z",
     "start_time": "2025-02-16T09:22:46.896234Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T15:26:44.816097Z",
     "iopub.status.busy": "2025-02-16T15:26:44.815567Z",
     "iopub.status.idle": "2025-02-16T15:26:44.819857Z",
     "shell.execute_reply": "2025-02-16T15:26:44.819024Z",
     "shell.execute_reply.started": "2025-02-16T15:26:44.816059Z"
    },
    "id": "WxsjkyQgN4yZ",
    "outputId": "5ce160c2-ce7e-4bc3-d2b6-a332c7fcf6a1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install Cython  # if failed to install fastBPE, try this line\n",
    "# !pip install fastBPE #分词使用\n",
    "# 在 Windows 系统上并没有 sys/mman.h 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T15:26:46.272468Z",
     "iopub.status.busy": "2025-02-16T15:26:46.271953Z",
     "iopub.status.idle": "2025-02-16T15:26:46.277754Z",
     "shell.execute_reply": "2025-02-16T15:26:46.276987Z",
     "shell.execute_reply.started": "2025-02-16T15:26:46.272412Z"
    },
    "id": "f6Ry1PcAn4k1",
    "outputId": "6393524e-e5c3-4f93-dd9d-f249ca52bc05",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate-transformer-not-share'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T03:10:32.326542400Z",
     "start_time": "2024-08-05T03:10:31.006376300Z"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T15:26:48.198291Z",
     "iopub.status.busy": "2025-02-16T15:26:48.197781Z",
     "iopub.status.idle": "2025-02-16T15:26:48.419006Z",
     "shell.execute_reply": "2025-02-16T15:26:48.418059Z",
     "shell.execute_reply.started": "2025-02-16T15:26:48.198252Z"
    },
    "id": "jT1Mqiq3N4yZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1171/1465594175.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"checkpoints/{exp_name}/best.ckpt\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(f\"checkpoints/{exp_name}/best.ckpt\", map_location=\"cpu\")\n",
    "\n",
    "# state_dict1 = torch.load(\"epoch125-step132426.ckpt\", map_location=\"cpu\")\n",
    "# state_dict = state_dict1[\"state_dict\"]\n",
    "\n",
    "# update keys by dropping `model`\n",
    "# for key in list(state_dict):\n",
    "#     state_dict[key.replace(\"model.\", \"\")] = state_dict.pop(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T03:10:46.720508800Z",
     "start_time": "2024-08-05T03:10:36.179313900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-02-16T15:01:09.546101Z",
     "iopub.status.idle": "2025-02-16T15:01:09.546978Z",
     "shell.execute_reply": "2025-02-16T15:01:09.546716Z",
     "shell.execute_reply.started": "2025-02-16T15:01:09.546688Z"
    },
    "id": "eKwIDdjgvTlO",
    "outputId": "35979ff2-a16d-4aa4-881a-8496e39e2469"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T15:26:53.831514Z",
     "iopub.status.busy": "2025-02-16T15:26:53.831006Z",
     "iopub.status.idle": "2025-02-16T15:26:53.995189Z",
     "shell.execute_reply": "2025-02-16T15:26:53.993768Z",
     "shell.execute_reply.started": "2025-02-16T15:26:53.831477Z"
    },
    "id": "WVksjIhL_pt9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r wmt16/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T15:26:55.158502Z",
     "iopub.status.busy": "2025-02-16T15:26:55.157888Z",
     "iopub.status.idle": "2025-02-16T15:26:55.166151Z",
     "shell.execute_reply": "2025-02-16T15:26:55.165372Z",
     "shell.execute_reply.started": "2025-02-16T15:26:55.158457Z"
    },
    "id": "2MY69-6WWSjp",
    "outputId": "911d9066-7917-4917-b5cc-d679a1c63865",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a man in an seinem hat sh-@@ at guitar .']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([[   5,   16,    6,   23,  150,   80, 8248,   35,  232,    4,    3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "c9db4cd83b7b4f53a0fa93b8c365debd",
      "a972aba7913b48c6973ae27bff5b9d1d",
      "22aa774ab39e4fd78b0ff08d430d2b5d",
      "9a01f02fb99b4b81ac487d23bc5daf4a",
      "19f846338f6b48c9930b58575641e5b8",
      "7d293204dc1b4d97bb5ee670787c7b7d",
      "ea8fa440f320435aa75c9bef75bc3f08",
      "4e64bcb87e584beda0408a0aa5a659ae",
      "b077d7562d814df5b6d839464dd4122f",
      "5a28babf4e2e4393bdc1a093034885ab",
      "be07d518c21149d196c610fd57b1eaec"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T15:26:58.235483Z",
     "iopub.status.busy": "2025-02-16T15:26:58.234974Z",
     "iopub.status.idle": "2025-02-16T15:27:14.538399Z",
     "shell.execute_reply": "2025-02-16T15:27:14.537619Z",
     "shell.execute_reply.started": "2025-02-16T15:26:58.235446Z"
    },
    "id": "5QgodlOJfgKj",
    "outputId": "2907bad2-7cb1-4295-c8b0-77ac0dabf398",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save cache to wmt16/.cache/de2en_test_128.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "6it [00:00, 57.03it/s]/usr/local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "20it [00:00, 64.50it/s]/usr/local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "966it [00:14, 65.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss: 3.317236665859973\n",
      "Average BLEU score: 0.5904505906133758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# 加载 Transformer 模型\n",
    "model = TransformerModel(config)  # 初始化 Transformer 模型\n",
    "model.load_state_dict(state_dict)  # 加载预训练模型的参数\n",
    "\n",
    "# 定义损失函数（交叉熵损失，带有标签平滑）\n",
    "loss_fct = CrossEntropyWithPadding(config)\n",
    "\n",
    "# 加载测试数据集\n",
    "test_ds = LangPairDataset(\"test\", max_length=128, data_dir=\"./wmt16\")  # 测试数据集\n",
    "test_dl = DataLoader(test_ds, batch_size=1, collate_fn=partial(collate_fct, tokenizer=tokenizer))  # 数据加载器\n",
    "\n",
    "# 迁移模型到计算设备（CPU/GPU）\n",
    "model = model.to(device)\n",
    "model.eval()  # 设置模型为评估模式，防止 dropout 或 batchnorm 影响推理\n",
    "\n",
    "# 定义数据收集字典\n",
    "collect = {}  # 用于存储测试集中的样本及其损失\n",
    "loss_collect = []  # 用于收集所有样本的损失\n",
    "\n",
    "predictions = []  # 存储所有预测结果\n",
    "answers = []  # 存储所有真实标签\n",
    "bleu_scores = []  # 存储 BLEU 评分\n",
    "\n",
    "# 遍历测试集\n",
    "for idx, batch in tqdm(enumerate(test_dl)):\n",
    "    encoder_inputs = batch[\"encoder_inputs\"]  # 编码器输入（源语言）\n",
    "    encoder_inputs_mask = batch[\"encoder_inputs_mask\"]  # 源输入的填充掩码\n",
    "    decoder_inputs = batch[\"decoder_inputs\"]  # 解码器输入（目标语言）\n",
    "    decoder_labels = batch[\"decoder_labels\"]  # 目标真实标签（用于计算损失）\n",
    "\n",
    "    # 进行前向传播，获取 Transformer 输出\n",
    "    outputs = model(\n",
    "        encoder_inputs=encoder_inputs,\n",
    "        decoder_inputs=decoder_inputs,\n",
    "        encoder_inputs_mask=encoder_inputs_mask\n",
    "    )\n",
    "\n",
    "    # 计算交叉熵损失\n",
    "    loss = loss_fct(outputs.logits, decoder_labels)\n",
    "\n",
    "    # 获取预测结果：取每个时间步上最大概率的词索引\n",
    "    preds = outputs.logits.argmax(dim=-1)  # 预测序列形状为 [1, seq_len]\n",
    "\n",
    "    # 将预测索引转换为实际的文本句子\n",
    "    preds = tokenizer.decode(preds.cpu().numpy())  # ['预测句子']\n",
    "    \n",
    "    # 将真实标签转换为文本句子\n",
    "    decoder_labels = tokenizer.decode(decoder_labels.cpu().numpy())  # ['标签句子']\n",
    "\n",
    "    # 计算 BLEU 评分，使用 1-gram 计算精确匹配\n",
    "    bleu = sentence_bleu([decoder_labels[0].split()], preds[0].split(), weights=(1, 0, 0, 0))\n",
    "    bleu_scores.append(bleu)  # 存储 BLEU 分数\n",
    "\n",
    "    # 记录样本信息，包括损失、输入、目标、预测\n",
    "    collect[idx] = {\n",
    "        \"loss\": loss.item(),  # 当前样本的损失\n",
    "        \"src_inputs\": encoder_inputs,  # 源语言输入\n",
    "        \"trg_inputs\": decoder_inputs,  # 目标语言输入\n",
    "        \"mask\": encoder_inputs_mask,  # 源语言的填充掩码\n",
    "        \"trg_labels\": decoder_labels,  # 真实目标文本\n",
    "        \"preds\": preds  # 预测文本\n",
    "    }\n",
    "\n",
    "    # 记录损失\n",
    "    loss_collect.append(loss.item())\n",
    "\n",
    "# 按照损失大小对收集的数据进行排序（从低到高）\n",
    "collect = sorted(collect.items(), key=lambda x: x[1][\"loss\"])\n",
    "\n",
    "# 输出测试集平均损失\n",
    "print(f\"testing loss: {np.array(loss_collect).mean()}\")\n",
    "\n",
    "# 计算测试集的平均 BLEU 评分\n",
    "print(f\"Average BLEU score: {sum(bleu_scores) / len(bleu_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:16:14.099749Z",
     "start_time": "2024-05-06T13:16:13.991811900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535,
     "referenced_widgets": [
      "40f7e3754ad44b33acfe0a80b3648a3a",
      "b1e51beac50f4947a9dd235d8aa87603",
      "8cd9e9702e114ebc856f56975544ede8",
      "b7c7a718a8654956947108969557ca83",
      "20a27d1ec76f4d85b1ffe99401fbb5b7",
      "e0ddf3970b3e4248bc87d4649fa9c448",
      "d7bdde54e2534c49a207540bd258d3ae",
      "22484000400842f0b6e245dc610acbba",
      "a66a5b4ff37d4dafa4ba3081aa1359f4",
      "0b7866c29e5f4afe9b92de15f1b88c3e",
      "333e84a304184c57af1821620ae80f6a"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-02-16T15:27:19.229440Z",
     "iopub.status.busy": "2025-02-16T15:27:19.228541Z",
     "iopub.status.idle": "2025-02-16T15:27:21.600184Z",
     "shell.execute_reply": "2025-02-16T15:27:21.599363Z",
     "shell.execute_reply.started": "2025-02-16T15:27:19.229385Z"
    },
    "id": "KGSym4CbN4ya",
    "outputId": "f5e9c4c2-c92e-44e6-c83a-5cb720305428",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vocabulary from ./wmt16/vocab ...\n",
      "Read 762259 words (18107 unique) from vocabulary file.\n",
      "Loading codes from ./wmt16/bpe.20000 ...\n",
      "Read 20001 codes from the codes file.\n",
      "  7%|▋         | 9/128 [00:00<00:02, 58.52it/s]\n",
      "/usr/local/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['man in white boat on a small boat.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from fastBPE import fastBPE\n",
    "from sacremoses import MosesDetokenizer, MosesTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# `MosesTokenizer` 和 `MosesDetokenizer` 来自 `sacremoses` 库，主要用于自然语言处理中的分词（Tokenization）和去标记化（Detokenization）。\n",
    "# `fastBPE` 用于执行 Byte Pair Encoding（BPE）分词技术，在神经机器翻译和 NLP 任务中非常常见。\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self, model, src_tokenizer, trg_tokenizer):\n",
    "        \"\"\"\n",
    "        初始化翻译器类，包含 BPE 处理、分词、去标记化和模型推理功能。\n",
    "\n",
    "        参数：\n",
    "        - model: Transformer 模型对象，用于执行翻译推理\n",
    "        - src_tokenizer: 源语言的分词器\n",
    "        - trg_tokenizer: 目标语言的分词器\n",
    "        \"\"\"\n",
    "        # 初始化 BPE 处理器，加载预训练的 BPE 词汇表\n",
    "        self.bpe = fastBPE(\"./wmt16/bpe.20000\", \"./wmt16/vocab\")\n",
    "        # 初始化 Moses 分词器（源语言为德语 \"de\"）\n",
    "        self.mose_tokenizer = MosesTokenizer(lang=\"de\")\n",
    "        # 初始化 Moses 去标记化工具（目标语言为英语 \"en\"）\n",
    "        self.mose_detokenizer = MosesDetokenizer(lang=\"en\")\n",
    "        # 存储模型并设置为评估模式\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # 存储源语言和目标语言的分词器\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        # 正则表达式，用于移除 BPE 生成的 `@@` 标记\n",
    "        self.pattern = re.compile(r'(@@ )|(@@ ?$)')\n",
    "\n",
    "    def draw_attention_map(self, attn_scores, cross_attn_scores, src_words_list, trg_words_list):\n",
    "        \"\"\"\n",
    "        绘制 Transformer 模型的注意力（Attention）热力图，包括自注意力和交叉注意力。\n",
    "\n",
    "        参数：\n",
    "        - attn_scores: 自注意力分数（形状：[num_heads, trg_len, trg_len]）\n",
    "        - cross_attn_scores: 交叉注意力分数（形状：[num_heads, trg_len, src_len]）\n",
    "        - src_words_list: 源语言单词列表\n",
    "        - trg_words_list: 目标语言单词列表\n",
    "        \"\"\"\n",
    "        # 确保输入张量具有正确的形状\n",
    "        assert len(attn_scores.shape) == 3, f\"attn_scores 形状应为 [num_heads, trg_len, trg_len]，但得到了 {attn_scores.shape}\"\n",
    "        attn_scores = attn_scores[:, :len(trg_words_list), :len(trg_words_list)]\n",
    "        assert len(cross_attn_scores.shape) == 3, f\"cross_attn_scores 形状应为 [num_heads, trg_len, src_len]，但得到了 {cross_attn_scores.shape}\"\n",
    "        cross_attn_scores = cross_attn_scores[:, :len(trg_words_list), :len(src_words_list)]\n",
    "\n",
    "        num_heads, trg_len, src_len = cross_attn_scores.shape\n",
    "\n",
    "        # 创建一个 10x5 的绘图窗口，并自动调整子图布局\n",
    "        fig = plt.figure(figsize=(10, 5), constrained_layout=True)\n",
    "        grid = plt.GridSpec(trg_len, trg_len + src_len, wspace=0.1, hspace=0.1)\n",
    "\n",
    "        # 自注意力热力图\n",
    "        self_map = fig.add_subplot(grid[:, :trg_len])\n",
    "        self_map.matshow(attn_scores.mean(dim=0), cmap='viridis')\n",
    "        self_map.set_yticks(range(trg_len), trg_words_list, fontsize=10)\n",
    "        self_map.set_xticks(range(trg_len), [\"[BOS]\"] + trg_words_list[:-1], rotation=90)\n",
    "\n",
    "        # 交叉注意力热力图\n",
    "        cross_map = fig.add_subplot(grid[:, trg_len:])\n",
    "        cross_map.matshow(cross_attn_scores.mean(dim=0), cmap='viridis')\n",
    "        cross_map.set_yticks(range(trg_len), [])\n",
    "        cross_map.set_xticks(range(src_len), src_words_list, rotation=90)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def __call__(self, sentence_list, heads_list=None, layer_idx=-1):\n",
    "        \"\"\"\n",
    "        执行翻译任务，并可选地绘制注意力热力图。\n",
    "\n",
    "        参数：\n",
    "        - sentence_list: 需要翻译的源语言句子列表\n",
    "        - heads_list: 选择要可视化的注意力头列表（可选）\n",
    "        - layer_idx: 选择可视化 Transformer 的哪一层（默认 -1，即最后一层）\n",
    "\n",
    "        返回：\n",
    "        - 翻译后的目标语言句子列表\n",
    "        \"\"\"\n",
    "        # 将句子转换为小写并进行分词\n",
    "        sentence_list = [\" \".join(self.mose_tokenizer.tokenize(s.lower())) for s in sentence_list]\n",
    "        # 进行 BPE 编码\n",
    "        tokens_list = [s.split() for s in self.bpe.apply(sentence_list)]\n",
    "        # 编码输入序列，并添加起始标记 ([BOS]) 和结束标记 ([EOS])\n",
    "        encoder_input, attn_mask = self.src_tokenizer.encode(\n",
    "            tokens_list,\n",
    "            add_bos=True,\n",
    "            add_eos=True,\n",
    "            return_mask=True,\n",
    "        )\n",
    "        # 将编码后的输入转换为 PyTorch 张量\n",
    "        encoder_input = torch.Tensor(encoder_input).to(dtype=torch.int64)\n",
    "        # 使用 Transformer 模型进行推理\n",
    "        outputs = self.model.infer(encoder_inputs=encoder_input, encoder_inputs_mask=attn_mask)\n",
    "\n",
    "        preds = outputs.preds.numpy()\n",
    "        # 解码目标语言句子\n",
    "        trg_decoded = self.trg_tokenizer.decode(preds, split=True, remove_eos=False, remove_bos=False, remove_pad=False)\n",
    "        # 解码源语言句子\n",
    "        src_decoded = self.src_tokenizer.decode(\n",
    "            encoder_input.numpy(),\n",
    "            split=True,\n",
    "            remove_bos=False,\n",
    "            remove_eos=False\n",
    "        )\n",
    "\n",
    "        # 绘制注意力热力图\n",
    "        for attn_score, cross_attn_score, src, trg in zip(\n",
    "            outputs.decoder_self_attn_scores[layer_idx], outputs.decoder_cross_attn_scores[layer_idx], src_decoded, trg_decoded\n",
    "        ):\n",
    "            if heads_list is None:\n",
    "                self.draw_attention_map(attn_score, cross_attn_score, src, trg)\n",
    "            else:\n",
    "                self.draw_attention_maps(attn_score, cross_attn_score, src, trg, heads_list=heads_list)\n",
    "\n",
    "        # 返回翻译结果，并去除 BPE 标记\n",
    "        return [self.mose_detokenizer.tokenize(self.pattern.sub(\"\", s).split()) for s in self.trg_tokenizer.decode(preds)]\n",
    "\n",
    "# 示例输入句子列表（德语）\n",
    "sentence_list = [\n",
    "    \"Mann in einem kleinen weißen Boot auf einem See.\",  # \"Man in a small white boat on a lake.\"\n",
    "]\n",
    "\n",
    " # \"Ein Mann mit einem Eimer und ein Mädchen mit einem Hut am Strand.\", # A man with a bucket and a girl in a hat on the beach.\n",
    "# \"Drei Männer auf Pferden während eines Rennens.\",  # Three men on horses during a race.\n",
    "# \"Ein Mann und eine Frau essen zu Abend\",  # 一个男人和一个女人在吃晚餐\n",
    "\n",
    "# 加载 Transformer 模型\n",
    "model = TransformerModel(config)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 初始化 Translator 对象\n",
    "translator = Translator(model.cpu(), tokenizer, tokenizer)\n",
    "\n",
    "# 执行翻译\n",
    "translator(sentence_list, layer_idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-02-16T15:01:09.554503Z",
     "iopub.status.idle": "2025-02-16T15:01:09.554868Z",
     "shell.execute_reply": "2025-02-16T15:01:09.554728Z",
     "shell.execute_reply.started": "2025-02-16T15:01:09.554712Z"
    },
    "id": "e0WkUQuUe-Cy",
    "outputId": "433e3d70-4f34-4376-e4a3-1b8834e71f88"
   },
   "outputs": [],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-16T15:01:09.555859Z",
     "iopub.status.idle": "2025-02-16T15:01:09.556224Z",
     "shell.execute_reply": "2025-02-16T15:01:09.556081Z",
     "shell.execute_reply.started": "2025-02-16T15:01:09.556065Z"
    },
    "id": "_1ZLtdahywWf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt: 把best.ckpt复制到云盘内\n",
    "# !cp -r checkpoints/translate-transformer-not-share/best.ckpt /content/drive/MyDrive/transformer-de-en"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00fe6a550df547cea5b1e0d84c1a3f5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83f345f7424c43929d7a304cbfb6d0fe",
      "max": 9714,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_58d69d086f644e89937347e1f9261d6f",
      "value": 9714
     }
    },
    "013d2a6b90144432bbfb15136235faba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b7866c29e5f4afe9b92de15f1b88c3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c0c7229347b49ffbb9a0f7bd41199b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e10428326bc436e8dbc079e89570bde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0fe4ed7283c04496affe727c235d31c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19f846338f6b48c9930b58575641e5b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20a27d1ec76f4d85b1ffe99401fbb5b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22484000400842f0b6e245dc610acbba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22aa774ab39e4fd78b0ff08d430d2b5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e64bcb87e584beda0408a0aa5a659ae",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b077d7562d814df5b6d839464dd4122f",
      "value": 1
     }
    },
    "2b82e42c34804b13bc83be4c7009d4e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "333e84a304184c57af1821620ae80f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40f7e3754ad44b33acfe0a80b3648a3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1e51beac50f4947a9dd235d8aa87603",
       "IPY_MODEL_8cd9e9702e114ebc856f56975544ede8",
       "IPY_MODEL_b7c7a718a8654956947108969557ca83"
      ],
      "layout": "IPY_MODEL_20a27d1ec76f4d85b1ffe99401fbb5b7"
     }
    },
    "434ac4c51db74681942d2d3bd869b1df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "46a51a1719d8444eb5877720253d3c37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e64bcb87e584beda0408a0aa5a659ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "55146ac7395840dca847e0f63aa80561": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_abb9bc76c0ce4634bd340be20c074b8c",
       "IPY_MODEL_a89d790df59245fcaf26335d4903c9b4",
       "IPY_MODEL_f837255914194dc792cf1514dee03dc5"
      ],
      "layout": "IPY_MODEL_46a51a1719d8444eb5877720253d3c37"
     }
    },
    "58d69d086f644e89937347e1f9261d6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5a28babf4e2e4393bdc1a093034885ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "673bd3773a374261a01cfc48081c27f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7190c928edb040c3a523cdd2e3f45572": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_990741ff083d457587665d8f620b69c3",
       "IPY_MODEL_00fe6a550df547cea5b1e0d84c1a3f5f",
       "IPY_MODEL_fab8b1e3187841ee894b92859fa4821b"
      ],
      "layout": "IPY_MODEL_f9b0d916e1a64c2a9f384e42f3d5dcab"
     }
    },
    "7d293204dc1b4d97bb5ee670787c7b7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83f345f7424c43929d7a304cbfb6d0fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cd9e9702e114ebc856f56975544ede8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22484000400842f0b6e245dc610acbba",
      "max": 128,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a66a5b4ff37d4dafa4ba3081aa1359f4",
      "value": 12
     }
    },
    "8cf462fa996c41b18fb4571e0a757363": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9233439bdd6746c28ad4a20ef47799f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "990741ff083d457587665d8f620b69c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a78199650ecc47bd81f482a2671b563a",
      "placeholder": "​",
      "style": "IPY_MODEL_673bd3773a374261a01cfc48081c27f3",
      "value": "100%"
     }
    },
    "9a01f02fb99b4b81ac487d23bc5daf4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a28babf4e2e4393bdc1a093034885ab",
      "placeholder": "​",
      "style": "IPY_MODEL_be07d518c21149d196c610fd57b1eaec",
      "value": " 957/? [00:18&lt;00:00, 51.31it/s]"
     }
    },
    "a66a5b4ff37d4dafa4ba3081aa1359f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a78199650ecc47bd81f482a2671b563a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a89d790df59245fcaf26335d4903c9b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9233439bdd6746c28ad4a20ef47799f6",
      "max": 280,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e10428326bc436e8dbc079e89570bde",
      "value": 280
     }
    },
    "a972aba7913b48c6973ae27bff5b9d1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d293204dc1b4d97bb5ee670787c7b7d",
      "placeholder": "​",
      "style": "IPY_MODEL_ea8fa440f320435aa75c9bef75bc3f08",
      "value": ""
     }
    },
    "abb9bc76c0ce4634bd340be20c074b8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_013d2a6b90144432bbfb15136235faba",
      "placeholder": "​",
      "style": "IPY_MODEL_2b82e42c34804b13bc83be4c7009d4e2",
      "value": ""
     }
    },
    "b077d7562d814df5b6d839464dd4122f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1e51beac50f4947a9dd235d8aa87603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0ddf3970b3e4248bc87d4649fa9c448",
      "placeholder": "​",
      "style": "IPY_MODEL_d7bdde54e2534c49a207540bd258d3ae",
      "value": "  9%"
     }
    },
    "b7c7a718a8654956947108969557ca83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b7866c29e5f4afe9b92de15f1b88c3e",
      "placeholder": "​",
      "style": "IPY_MODEL_333e84a304184c57af1821620ae80f6a",
      "value": " 12/128 [00:00&lt;00:01, 73.96it/s]"
     }
    },
    "be07d518c21149d196c610fd57b1eaec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9db4cd83b7b4f53a0fa93b8c365debd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a972aba7913b48c6973ae27bff5b9d1d",
       "IPY_MODEL_22aa774ab39e4fd78b0ff08d430d2b5d",
       "IPY_MODEL_9a01f02fb99b4b81ac487d23bc5daf4a"
      ],
      "layout": "IPY_MODEL_19f846338f6b48c9930b58575641e5b8"
     }
    },
    "d7bdde54e2534c49a207540bd258d3ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0ddf3970b3e4248bc87d4649fa9c448": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea8fa440f320435aa75c9bef75bc3f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f837255914194dc792cf1514dee03dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fe4ed7283c04496affe727c235d31c4",
      "placeholder": "​",
      "style": "IPY_MODEL_0c0c7229347b49ffbb9a0f7bd41199b9",
      "value": " 21020/? [21:30&lt;00:00, 15.32it/s, epoch=19, loss=2.61, val_loss=3.41]"
     }
    },
    "f9b0d916e1a64c2a9f384e42f3d5dcab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fab8b1e3187841ee894b92859fa4821b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cf462fa996c41b18fb4571e0a757363",
      "placeholder": "​",
      "style": "IPY_MODEL_434ac4c51db74681942d2d3bd869b1df",
      "value": " 9714/9714 [00:00&lt;00:00, 236744.37it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
